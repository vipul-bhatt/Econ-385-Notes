# Multiple regression model

In the previous chapter we modeled wages as a function of education only. However, it is reasonable to argue that there are other characteristics of individuals that can affect their wages. For example, industry of employment can affect wages with those in financial industry earning a higher wage on average than those engaged in retail industry. More importantly,  there is an argument for this variable having an effect on wages which is independent of education. As a result, ignoring this variable will **confound** the effect of education on wages. In order to compute the pure effect of education on wages, we must compare those employed in the same industry but have different levels of education. This is what we mean by **controlling** for other factors in the regression and it is closely related to the concept of *ceteris paribus (all else equal)* in economics.

```{block2, type="rmdNote"}

Note that ther can be two types of confounding factors:

1. Observed confounding factors: these, as the name suggests, are measurable relevant variables that can affect the outcome variable of interest. The idea here is that two observations may differ across obersvable dimensions which in turn influences the difference in their outcomes. This is known as **observed heterogeneity**. The multiple regression model can address this problem by adding all relevant and measurable independent variables to the right hand side of the regression model.



2. Unobserved confounding factors: these variables by definition are unobserved or difficult to measure in real world. The idea here is that two observations may differ across unobersvable dimensions which in turn influences the difference in their outcomes. This is known as **unobserved heterogeneity**. Going back to our wages regression model, it is possible that two inidividuals with same level of education and industry of employment earn different wages due to differences in their innate ability which is unobserved and very difficult to measure.  There are some methods in Econometrics that address this problem and this problem posese one of the most serious challenge to the credibility of the estimated regression cofficients using the OLS. More on this later.

```

## Multiple Regression Model

The multiple regression model simply adds more independent variables on the right-hand side of the regression model. Suppose $Y_i$ denotes our outcome variable of interest for observation $i$ and we believe there are $K$ possible determinants of this outcome, denoted by $X_{i1}, X_{i2},...,X{iK}$. The population regression function (PRF), which is the conditional expectation of $Y_i$ given data on all $X$ variables is given by:


\[E(Y_i|X_{1i},X_{2i},...,X_{Ki})=f(X_{1i},X_{2i},...,X_{Ki})\]

The resulting regression model is called **multiple regression** model:

\[Y_i=E(Y_i|X_{1i},X_{2i},...,X_{Ki})+\epsilon_i\]

Now we need to make an assumption about how each $X$ variable affects the dependent variable. For example, if all effects are linear, then we get the following regression model:

\[Y_i=\beta_0 +\beta_1X_{1i} + \beta_2 X_{2i}+...+\beta_KX_{Ki}+\epsilon_i\]


An important distinction from the simple regression model is the way we interpret the slope coefficients. Now, $\beta_1$ captures the effect of a unit change in $X_1$ on $Y$, **\textbf{holding all other X-variables constant}. In this sense, $\beta_1$ is the partial slope as it measures the pure effect of $X_1$ on $Y$ after partialing or netting out the effects of all other included X-variables on $Y$.

For the above model, the predicted value of $Y_i$ is given by:

\[\hat{Y_i}=\hat{\beta_0} +\hat{\beta_1}X_{1i} + \hat{\beta_2} X_{2i}+...+\hat{\beta_K}X_{Ki}\]

and the residuals are given by:

\[e_i = Y_i- \hat{Y_i}= Y_i - (\hat{\beta_0} +\hat{\beta_1}X_{1i} + \hat{\beta_2} X_{2i}+...+\hat{\beta_K}X_{Ki})\]

Note that the OLS minimization problem now involves choosing $\hat{\beta_0}, \hat{\beta_1}, \hat{\beta_2},..., \hat{\beta_K}$ that will minimize the sum of residuals squared (RSS) where
\[RSS=\sum_{i=1}^N e_i^2=\sum_{i=1}^N [Y_i - (\hat{\beta_0} +\hat{\beta_1}X_{1i} + \hat{\beta_2} X_{2i}+...+\hat{\beta_K}X_{Ki})]^2 \]


```{example}

Suppose we are interested in finding out what labor market characteristics affect wages. For this purpose, we collect data on wages, education and experience. Suppose the regression model is given by:

\[ln(Wage_i)= \beta_0 +\beta_1 Education_i + \beta_2 Experience_i + \epsilon_i\]

where $Wage_i$ is the annual wages and salaries of individual $i$ in dollars. $Education_i$ denotes years of education of individual $i$. Finally, $Experience_i$ denotes years of experience.


Table \@ref(tab:ch4table1) below presents the estimation results for the extended wage regression. Note that now the interpretation of the coefficient of education is as follows: holding the level of experience constant at its mean, raising education by 1 year will increase wages by 11.3\%.  Also notice that the coefficient of experience is positive indicating that more experienced workers earn higher wage on average.
```


```{r ch4table1,eval=knitr::is_html_output(), message=FALSE, warning=FALSE, echo=FALSE}

library(ipumsr)

ddi = read_ipums_ddi("C:/Users/bhattvx/Dropbox/Ebooks/Econ-385-Notes/datafiles/cps_00010.xml")
data =read_ipums_micro(ddi, verbose=F)

##### drop missing  work hours

data=subset(data, UHRSWORK1!=999)
data=subset(data, UHRSWORK1!=997)
data=subset(data, UHRSWORK1!=0)

### drop missing wages

data=subset(data,INCWAGE!=9999999)
data=subset(data,INCWAGE!=9999998)
data=subset(data,INCWAGE!=0)
## generate schooling data
data$schooling=NA
data$schooling[data$EDUC==011]=1
data$schooling[data$EDUC==012]=2
data$schooling[data$EDUC==013]=3
data$schooling[data$EDUC==014]=4
data$schooling[data$EDUC==021]=5
data$schooling[data$EDUC==022]=6
data$schooling[data$EDUC==031]=7
data$schooling[data$EDUC==032]=8
data$schooling[data$EDUC==040]=9
data$schooling[data$EDUC==050]=10
data$schooling[data$EDUC==060]=11
data$schooling[data$EDUC==073]=12
data$schooling[data$EDUC==080]=13
data$schooling[data$EDUC==090]=14
data$schooling[data$EDUC==100]=15
data$schooling[data$EDUC==111]=16
data$schooling[data$EDUC==123]=18
data$schooling[data$EDUC==125]=21

### drop NA from schooling
data=na.omit(data)

### keep full time workers only

data=subset(data,UHRSWORK1>=40)

## only look at first 1000 observations
data=data[1:1000,]


y=log(data$INCWAGE)
x1=data$schooling
x2=data$AGE-data$schooling-6
x3=x2^2
fit1=lm(y~x1)
fit2=lm(y~x1+x2)


suppressMessages(library(sjPlot))
suppressMessages(library(sjmisc))
suppressMessages(library(sjlabelled))

tab_model(fit1,fit2, digits=3, CSS = list(css.table='border: solid 2px;', css.td='border: solid 1px;',css.thead='font-weight: bold;', css.col2='text-align: center;', css.col3='text-align:center;', css.col4='text-align: center;', css.col5='text-align: center;', css.depvarhead = 'font-weight:bold;',css.firsttablerow='border-top: solid 2px;',  css.firstsumrow= 'border-top:2px solid;'), show.ci=F, show.loglik=T,p.style="numeric", show.obs=T, show.se=T, show.stat=T,dv.labels=c("Model 1","Model 2") , pred.labels = c("Intercept","Education", "Experience"), string.p="p-value", string.se="s.e.(b)", string.est="b", string.stat="t-ratio", string.pred="Explanatory Variables", col.order = c("est", "se", "stat", "p"))

```
<caption> (\#tab:ch4table1) OLS Estimation of Earnings Equation </caption>

```{r ch4table1a,eval=knitr::is_latex_output(), message=FALSE, warning=FALSE, echo=FALSE, results='asis'}

library(ipumsr)

ddi = read_ipums_ddi("C:/Users/bhattvx/Dropbox/Ebooks/Econ-385-Notes/datafiles/cps_00010.xml")
data =read_ipums_micro(ddi, verbose=F)

##### drop missing  work hours

data=subset(data, UHRSWORK1!=999)
data=subset(data, UHRSWORK1!=997)
data=subset(data, UHRSWORK1!=0)

### drop missing wages

data=subset(data,INCWAGE!=9999999)
data=subset(data,INCWAGE!=9999998)
data=subset(data,INCWAGE!=0)
## generate schooling data
data$schooling=NA
data$schooling[data$EDUC==011]=1
data$schooling[data$EDUC==012]=2
data$schooling[data$EDUC==013]=3
data$schooling[data$EDUC==014]=4
data$schooling[data$EDUC==021]=5
data$schooling[data$EDUC==022]=6
data$schooling[data$EDUC==031]=7
data$schooling[data$EDUC==032]=8
data$schooling[data$EDUC==040]=9
data$schooling[data$EDUC==050]=10
data$schooling[data$EDUC==060]=11
data$schooling[data$EDUC==073]=12
data$schooling[data$EDUC==080]=13
data$schooling[data$EDUC==090]=14
data$schooling[data$EDUC==100]=15
data$schooling[data$EDUC==111]=16
data$schooling[data$EDUC==123]=18
data$schooling[data$EDUC==125]=21

### drop NA from schooling
data=na.omit(data)

### keep full time workers only

data=subset(data,UHRSWORK1>=40)

## only look at first 1000 observations
data=data[1:1000,]


y=log(data$INCWAGE)
x1=data$schooling
x2=data$AGE-data$schooling-6
x3=x2^2
fit1=lm(y~x1)
fit2=lm(y~x1+x2)


suppressMessages(library(stargazer))
stargazer(fit1,fit2, type="latex", header=FALSE, report="vc*s", covariate.labels =c("Intercept","Education", "Experience"), dep.var.labels="Log of Annual Wages", intercept.top = T, intercept.bottom = F,title="OLS Estimation of Earnings Function" )

```




## Goodness of fit redux

Table  \@ref(tab:ch4table1) shows results for two models: a simple regression model with education as the only explanatory model and a multiple regression model that adds experience to the simple model. A natural question to ask is whether adding these variables **matter** for explaining data on wages. As discussed in Chapter 3, $R^2$ provides one such measure. Higher value indicates greater variation of the dependent variable is explained by our included X-variables. However, we cannot compare these two models based on only $R^2$. The reason is by definition $R^2$ is guaranteed to increase when we add more independent variables to a model. This is true regardless of whether the variables we add our relevant to the problem at hand or not. In order to compare these two models with different number of explanatory variables, we need to acknowledge the following two consequences of adding an X-variable to our model:

i. The residual variation (unexplained) goes down improving the fit of the line. This is the **benefit** of adding an X-variable to the model.

ii. The degrees of freedom decreases by 1. This loss makes our estimators less precise and also lowers the power of any hypothesis test we may wish to conduct. This is the **cost** of adding an X-variable.

When we use $R^2$ to compare models with different number of explanatory variables, we are only considering the benefit of doing so without any regard to the cost. A more balanced measure would account for both the benefit in terms of improved fit and the cost in terms of the loss of the degrees of freedom. **Adjusted-$R^2$** is one such measure and is given by:

\[\text{Adjusted-}R^2= 1-\left(\frac{RSS}{TSS}\right)\times \left(\frac{N-1}{N-K}\right)\]

For a given sample size, as we add more X-variables, K increases. In the above formula, RSS goes down and hence the first term in the parenthesis given by $\left(\frac{RSS}{TSS}\right)$ becomes smaller. But  N-K goes down  as well and the second term in the parenthesis given by $\left(\frac{N-1}{N-K}\right)$ becomes smaller.  The net effect on **Adjusted-$R^2$** depends on which of these two effects, capturing benefit and cost previously defined, is stronger. As a result, unlike $R^2$, this measure does not increase just by adding more X-variables to the model.

As a result, we can compare the two models presented in table using adjusted-$R^2$ with a greater value indicating better fit. From Table  \@ref(tab:ch4table1) we observe that adding experience to the model increase the fit from 12.1\% to 15.8\%, indicating that including experience allows us to explain greater amount of observed variation in wages in our sample.

## Hypothesis testing in a multiple regression model

In addition to the test of statistical significance for each included X-variable in our model, we can now conduct different kinds of tests driven by economic theory. In this section we will use the production function and economic theory to illustrate hypothesis testing in a multiple regression framework. 

```{example, name="Cobb-Douglas Production Function"}
Consider the following Cobb-Douglas production function where output (Y) is a function of labor (L),  capital (K)  and material (M). In addition there is a constant that captures existing technology (A):
  
\[Y_i= A \times K_i^{\beta_1} \times  L^{\beta_2} \times M^{\beta_3}\]

Note that the value of $\beta_1+\beta_2+\beta_3$ captures the **economies of scale** with three possibilities:
  
1. Constant returns to scale implied by $\beta_1+\beta_2+\beta_3=1$

2. Decreasing returns to scale implied by $\beta_1+\beta_2+\beta_3<1$
  
3. Increasing returns to scale implied by $\beta_1+\beta_2+\beta_3>1$


Using the multiple regression model, we can easily estimate the above production function and formally test what kind of returns to scale are prevalent in our sample. To do that first rewrite the above production function in natural logs as follows:
  
  
\[ln(Y_i) = ln(A) + \beta_1 ln (K_i) + \beta_2 ln (L_i)+\beta_3 ln (M_i)\]


This shows that a Cobb-Douglas production function can be approximated by a relationship which linear in natural logs of output, labor, captial, and material. The implied regression model is given by:
  
\[ln(Y_i) = \beta_0 + \beta_1 ln (K_i) + \beta_2 ln (L_i) +\beta_3 ln (M_i) \epsilon_i\]

We can estimate the production function either by using a cross-sectional data on firms at a point in time, or use data of one firm over time. In this example, we will use a cross-section of 50 firms to estimate the production function.

Table  \@ref(tab:ch4table2) provides the results of the estimation of the above model. From the p-values for each slope coefficient, we can infer that each effect is statistically significant at the 5\% level of significance. In terms of economic significance, we find that 1\% increase in capital increases output by 0.34\%, holding constant labor and material inputs at their mean. In contrast, a 1\% increase in labor increases output by 0.51\%, holding constal capital and labor at their means. Finally, holding labor and capital constant, a 1\% increase in material input raises output by 0.25\%. Based on this we can argue that labor input has most economic significance. We also find that our model can explain 71.9\% of the total variation in output observed in our sample as indicated by $R^2$ value. Next, we will conduct two types of hypotheses which are only possible in a multiple regression case.

```


```{r ch4table2,eval=knitr::is_html_output(), message=FALSE, warning=FALSE, echo=FALSE}

### simulate data for production function

set.seed(423)
k=sample(1:100, 50, replace=FALSE)
l=sample(1:100, 50, replace=FALSE)
m=sample(1:100, 50, replace=FALSE)
z=sample(1:100, 50, replace=FALSE)

  
  
beta0=3.5
beta1=0.31
beta2=0.54
beta3=0.22
beta4=0.39

y= beta0+beta1*k +beta2*l + beta3*m+beta4*z+rnorm(50)

fit=lm(y~k+l+m)



### regression table using sjplot package

suppressMessages(library(sjPlot))
suppressMessages(library(sjmisc))
suppressMessages(library(sjlabelled))

tab_model(fit, digits=3, CSS = list(css.table='border: solid 2px;', css.td='border: solid 1px;',css.thead='font-weight: bold;', css.col2='text-align: center;', css.col3='text-align:center;', css.col4='text-align: center;', css.col5='text-align: center;', css.depvarhead = 'font-weight:bold;',css.firsttablerow='border-top: solid 2px;',
    css.firstsumrow= 'border-top:2px solid;'), show.ci=F, show.loglik=T,p.style="numeric", show.obs=T, show.se=T, show.stat=T,dv.labels="Dependent Variable: Natural Log of Output" , pred.labels = c("Intercept","Natural Log of Capital", "Natural Log of Labor", "Natural Log of Material"), string.p="p-value", string.se="s.e.(b)", string.est="b", string.stat="t-ratio", string.pred="Explanatory Variables", col.order = c("est", "se", "stat", "p"))

```
<caption> (\#tab:ch4table2) OLS Estimation of Cobb-Douglas Production Function </caption>

```{r ch4table2a, eval=knitr::is_latex_output(), message=FALSE, warning=FALSE, echo=FALSE, results='asis'}

### simulate data for production function

set.seed(423)
k=sample(1:100, 50, replace=FALSE)
l=sample(1:100, 50, replace=FALSE)
m=sample(1:100, 50, replace=FALSE)
z=sample(1:100, 50, replace=FALSE)

  
  
beta0=3.5
beta1=0.31
beta2=0.54
beta3=0.22
beta4=0.39

y= beta0+beta1*k +beta2*l + beta3*m+beta4*z+rnorm(50)

fit=lm(y~k+l+m)



### regression table using stargazer

suppressMessages(library(stargazer))
suppressMessages(library(stargazer))
stargazer(fit, type="latex", header=FALSE, report="vc*s", covariate.labels =c("Intercept","Natural Log of Capital", "Natural Log of Labor", "Natural Log of Material"), dep.var.labels="Natural Log of Output", intercept.top = T, intercept.bottom = F,title="OLS Estimation of Cobb-Douglas Production Function" )


```





### Test of statistical significance of the entire model

Here our goal is to test whether all slope coefficients included in our model are jointly equal to 0. If true, then our model does not add any explanation for the output. Formally, in our example, we are testing:

\[H_0: \beta_1=\beta_2=\beta_3=0\]
\[H_A: \ \text{Not} \ H_0\]

One way to think of the above null hypothesis is that it places a **restriction** on our model. If this condition is true then our model can be reduced to:

\[ln(Y_i) =\beta_0 + \epsilon_i\]

The above model is called the **restricted** model and it will gives a value of $R^2$ which we will denote it $R^2_R$. Note that because there is no  variable in our restricted model here, $R^2_R=0$ by definition. Our original model with labor, capital, and material included also gives us an $R^2$ which we will denote by $R^2_U$. If the null hypothesis is true, then dropping labor and capital from the model should not lead to a **significant** decline in goodness of fit (or $R^2$), i.e, the difference between $R^2_U$ and $R^2_R$ should not be large. Our test statistic for this test is given by:

\[F =\frac{(R^2_U - R^2_R)/J}{(1-R^2_U)/(N-K-1)} \]

where $J$ denotes the number of restrictions we impose in our null hypothesis and $K$ denotes number of X-variable in the original model. In this example both $J$ and $K$ take the value of 3.  Under the null hypothesis, this F statistic follows F-distribution with $J$ degrees of freedom for the numerator and $N-K-1$ degrees of freedom for the denominator. We can use the F-distribution table to get the critical value denoted by $F_c$. The decision rule is if $F>F_c$ then reject the null hypothesis. 

In our example, $R^2_U=0.719$. Also note that in this case, $R^2_R=0$. Hence, the F statistic value is:

\[F=\frac{(0.719-0)/3}{(1-0.719)/(50-3-1)}=39.32\]

The critical value at 5\% level of significance can be obtained from the F-distribution table using $df_1=J=3$ and  $df_2=N-K-1=46$. In our case this value is 2.81. Because the F statistic is greater than the critical value, we reject the null hypothesis and conclude that our model is able to explain a statistically significant amount of variation in output.

### Testing whether one or more of the variables can be eliminated from the model

In the previous example, we imposed the restriction that all three inputs have zero effect on output. However, often we maybe interested in finding out whether any one variable or a subset of included X-variables  in the model can be dropped without significantly impacting the fit of the model. For example, suppose we want to test whether dropping capital and material inputs from our model leads to a significant loss in the fit of the model. Here, our hypotheses are:

\[H_0: \beta_1=\beta_3=0 \]
\[H_A: \text{Not} \ H_0 \]

Our restricted model, if the statement in the $H_0$ is true is given by:

\[ln(Y_i) = \beta_0 + \beta_2 ln (L_i) +\epsilon_i\]

Note that if we estimate the above restricted model, we will obtain an $R^2$ value which will be non-zero but lower than the original model with capital and material inputs added. We can test whether the reduction in $R^2$ is significant or not. Table \@ref(tab:ch4table3) below presents the results of for the restricted model. For convenience, I also add the original unrestricted model  results from Table \@ref(tab:ch4table2) as well. We note that $R^2_U=0.719$ and  $R^2_R=0.414$. We can compute the F statistic as follows:

\[F=\frac{(0.719-0.414)/2}{(1-0.719)/(50-3-1)}=24.96\]

```{block2, type="rmdCaution"}
Note how in this example, we use $J=2$ and $K=3$ in the F statistic formula. This is because we are now dropping two variables from our orignal model implying that we are imposing two restrictions and hence $J=2$.}
```
The critical value at 5\% level of significance and $df_1=2, df_2=46$ is 3.20. Because the F statistic is greater than the critical value, we reject the null hypothesis and conclude that capital and material input add significantly to explanation of output variation in our sample.

```{r ch4table3 , eval=knitr::is_html_output(), message=FALSE, warning=FALSE, echo=FALSE}


### simulate data for production function

set.seed(423)
k=sample(1:100, 50, replace=FALSE)
l=sample(1:100, 50, replace=FALSE)
m=sample(1:100, 50, replace=FALSE)
z=sample(1:100, 50, replace=FALSE)

  
  
beta0=3.5
beta1=0.31
beta2=0.54
beta3=0.22
beta4=0.39

y= beta0+beta1*k +beta2*l + beta3*m+beta4*z+rnorm(50)

fit1=lm(y~k+l+m)
fit2=lm(y~l)


### regression table using sjplot package

suppressMessages(library(sjPlot))
suppressMessages(library(sjmisc))
suppressMessages(library(sjlabelled))

tab_model(fit1,fit2, digits=3, CSS = list(css.table='border: solid 2px;', css.td='border: solid 1px;',css.thead='font-weight: bold;', css.col2='text-align: center;', css.col3='text-align:center;', css.col4='text-align: center;', css.col5='text-align: center;', css.depvarhead = 'font-weight:bold;',css.firsttablerow='border-top: solid 2px;',
    css.firstsumrow= 'border-top:2px solid;'), show.ci=F, show.loglik=T,p.style="numeric", show.obs=T, show.se=T, show.stat=T,dv.labels=c("Unrestricted Model", "Restricted Model") , pred.labels = c("Intercept","Natural Log of Capital", "Natural Log of Labor", "Natural log of Material"), string.p="p-value", string.se="s.e.(b)", string.est="b", string.stat="t-ratio", string.pred="Explanatory Variables", col.order = c("est", "se", "stat", "p"))

```
<caption> (\#tab:ch4table3) OLS Estimation of Cobb-Douglas Production Function </caption>

```{r ch4table3a , eval=knitr::is_latex_output(), message=FALSE, warning=FALSE, echo=FALSE, results='asis'}


### simulate data for production function

set.seed(423)
k=sample(1:100, 50, replace=FALSE)
l=sample(1:100, 50, replace=FALSE)
m=sample(1:100, 50, replace=FALSE)
z=sample(1:100, 50, replace=FALSE)

  
  
beta0=3.5
beta1=0.31
beta2=0.54
beta3=0.22
beta4=0.39

y= beta0+beta1*k +beta2*l + beta3*m+beta4*z+rnorm(50)

fit1=lm(y~k+l+m)
fit2=lm(y~l)


### regression table using sjplot package

suppressMessages(library(stargazer))

stargazer(fit1,fit2, type="latex", header=FALSE, report="vc*s", covariate.labels =c("Intercept","Natural Log of Capital", "Natural Log of Labor", "Natural log of Material"), dep.var.labels="Natural Log of Output", intercept.top = T, intercept.bottom = F,title="OLS Estimation of Cobb-Douglas Function" )

```



### Testing a linear restriction on slope coefficients

Finally, we can also use our multiple regression model to test a linear restriction on the slope coefficients. Often such restrictions are informed by economic theory and vary from applications to applications. In our example, one such restriction is the returns to scale. Formally in our original model we can test the following hypotheses,

\[H_0: \beta_1+\beta_2 +\beta_3=1\]

\[ H_A: \beta_1+\beta_2 +\beta_3 \neq 1\]

If the null hypothesis is true, then we have constant returns to scale. Also notice that we are using a two-sided alternative hypothesis here. As result, rejection of the null hypothesis will imply non-constant returns to scale. But we cannot infer whether we have decreasing or increasing returns. For that we need to specify an appropriate one-sided alternative. For example, for decreasing returns to scale as our alternative, we would specify:

\[ H_A: \beta_1+\beta_2 +\beta_3 < 1\]


In this example, we will be using a two-sided alternative. There are two ways of conducting this kind of a hypothesis test.

- **Method 1: t-test**

Here, we can compute the t-ratio given by:

\[t= \frac{(\hat{\beta_1}+\hat{\beta_2} +\hat{\beta_3})-1}{s.e.(\hat{\beta_1}+\hat{\beta_2} +\hat{\beta_3})}\]

Note that Table \@ref(tab:ch4table2) gives us the three estimated regression coefficients. But in order to compute the standard error we also need pairwise correlations between estimated regression coefficients. This is because by definition:

\[s.e.(\hat{\beta_1}+\hat{\beta_2} +\hat{\beta_3})=\sqrt{Var(\hat{\beta_1})+Var(\hat{\beta_2})+Var(\hat{\beta_3})+2Cov(\hat{\beta_1},\hat{\beta_2})+2Cov(\hat{\beta_1},\hat{\beta_3})+ 2Cov(\hat{\beta_2},\hat{\beta_3})}\]

The covariance matrix for 3 regression coefficients is presented in Table \@ref(tab:ch4table4). The diagonal elements of this matrix gives us the variance of each coefficient whereas the off diagonal elements gives us the covariance between a pair of the coefficients. We can use that information and compute the standard error as follows:


\[s.e.(\hat{\beta_1}+\hat{\beta_2} +\hat{\beta_3})= \sqrt{0.0041+0.0039+0.0031+2\times (-0.0004)+ 2 \times (-0.002) + 2 \times 0.0006 }=0.105\]


Then, the t-statistic is given by:

\[t= \frac{(0.343+0.514+0.246)-1}{0.105}=0.981\]

Under the null hypothesis, our test statistic follows t-distribution with $N-K-1=46$ degrees of freedom. The critical value at 5\% level of significance for a two-sided alternative is 2.009. Because |t| is less than the critical value, we do not reject the null hypothesis. Hence, in our sample we do not find evidence against the constant returns to scale assumption.

```{r ch4table4, echo=F, warning=FALSE, message=FALSE}
library(knitr)
### simulate data for production function

set.seed(423)
k=sample(1:100, 50, replace=FALSE)
l=sample(1:100, 50, replace=FALSE)
m=sample(1:100, 50, replace=FALSE)
z=sample(1:100, 50, replace=FALSE)

  
  
beta0=3.5
beta1=0.31
beta2=0.54
beta3=0.22
beta4=0.39

y= beta0+beta1*k +beta2*l + beta3*m+beta4*z+rnorm(50)

fit=lm(y~k+l+m)
mat=vcov(fit)

colnames(mat)=c("$\\hat{\\beta_0}$","$\\hat{\\beta_1}$","$\\hat{\\beta_2}$","$\\hat{\\beta_3}$")
rownames(mat)=c("$\\hat{\\beta_0}$","$\\hat{\\beta_1}$","$\\hat{\\beta_2}$","$\\hat{\\beta_3}$")

kable(mat, caption = "Variance-Covariance Matrix", digits=4)
```

- **Method 2: F-test**

An alternative way to test a linear restriction is to follow the same method we used for previous two tests. We can use the null hypothesis restriction and rewrite our original model as:

\[ln(Y_i) = \beta_0 + (1-\beta_2-\beta_3) ln (K_i) + \beta_2 ln (L_i) +\beta_3 ln (M_i) \epsilon_i\]

or equivalently,

\[ln(Y_i)-ln(K_i) = \beta_0+ \beta_2 [ln (L_i)-ln(K_i)] +\beta_3 [ln (M_i)-ln(K_i)]+ \epsilon_i\]

Note here we have replaced $\beta_1=1- \beta_2-\beta_3$ using the linear restriction imposed by the null hypothesis. Consequently, the model specification written above is our restricted model and we can use the F-test to investigate whether imposing this restriction leads to a significant reduction in the model fit. The corresponding F-statistic in this case is given by 0.957. The critical value with $df_1=1$ and $df_2=46$ is 4.05. Because the F statistic is less than the critical value, we do not reject the null hypothesis.


## Problems{-}

**Exercise 4.1.** In a multiple regression analysis, a model has three independent variables. The analyst decides to add another (fourth) independent variable while retaining the other three independent variables. What will happen to  $R^2$ due to this addition? Explain.

**Exercise 4.2.** Suppose you have the following estimated model:



```{r extab1,eval=knitr::is_latex_output(), message=FALSE, warning=FALSE, echo=FALSE, results="asis"}

### simulate data for production function
set.seed(234)

x1 <- 11:50
x2 <- runif(40,4,82)
x3 <- abs(rnorm(40,3,15))

b0 <- 17
b1 <- 0.65
b2 <- 0.35
b3=-0.04

sigma <- 7.4

eps <- rnorm(x1,0,sigma)
y <- b0 + b1*x1  + b2*x2  + b3*x3 + eps

fit1=lm(y~x1+x2+x3)
fit2=lm(y~x1+x2)


### regression table using sjplot package

suppressMessages(library(stargazer))
stargazer(fit1,fit2, type="latex", header=FALSE, report="vc*s", covariate.labels =c("Intercept","Natural Log of Income", "Natural Log of Wealth", "Interest (%)"), dep.var.labels="Natural Log of Output", intercept.top = T, intercept.bottom = F,title="OLS Estimation of Cobb-Douglas Function" )

```


## Solutions{-}

**Exercise 4.1.** As discussed in section 4.2, mathematically RSS of the bigger model (with 4 variables) has to be lower than the RSS of the smaller model (with 3 variables). This is because OLS chooses slope coefficients by minimizing RSS. Now as we know that \[R^2=1-\frac{RSS}{TSS}\], mechanically a lower RSS value implies a larger $R^2$.   So in our example, an additional variable added to the model will lead to an increase in R-squared regardless of whether  the additional variable is relevant to the model or not. 