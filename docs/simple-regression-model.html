<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Simple Regression Model | Introduction to Econometrics</title>
  <meta name="description" content="Lecture notes for Introduction to Econometrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Simple Regression Model | Introduction to Econometrics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for Introduction to Econometrics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Simple Regression Model | Introduction to Econometrics" />
  
  <meta name="twitter:description" content="Lecture notes for Introduction to Econometrics" />
  

<meta name="author" content="Vipul Bhatt" />


<meta name="date" content="2023-08-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="multiple-regression-model.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Econometrics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="simple-regression-model.html"><a href="simple-regression-model.html"><i class="fa fa-check"></i><b>1</b> Simple Regression Model</a>
<ul>
<li class="chapter" data-level="1.1" data-path="simple-regression-model.html"><a href="simple-regression-model.html#statistical-foundation-of-the-simple-regression-model"><i class="fa fa-check"></i><b>1.1</b> Statistical foundation of the simple regression model</a></li>
<li class="chapter" data-level="1.2" data-path="simple-regression-model.html"><a href="simple-regression-model.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>1.2</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="1.3" data-path="simple-regression-model.html"><a href="simple-regression-model.html#goodness-of-fit"><i class="fa fa-check"></i><b>1.3</b> Goodness of fit</a></li>
<li class="chapter" data-level="1.4" data-path="simple-regression-model.html"><a href="simple-regression-model.html#applications-of-the-simple-regression-model"><i class="fa fa-check"></i><b>1.4</b> Applications of the simple regression model</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="simple-regression-model.html"><a href="simple-regression-model.html#aggregate-consumption-function"><i class="fa fa-check"></i><b>1.4.1</b> Aggregate consumption function</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple-regression-model.html"><a href="simple-regression-model.html#returns-to-education"><i class="fa fa-check"></i><b>1.4.2</b> Returns to education</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple-regression-model.html"><a href="simple-regression-model.html#use-of-natural-logs-and-interpretation-of-the-slope-coefficients"><i class="fa fa-check"></i><b>1.5</b> Use of natural logs and interpretation of the slope coefficients</a></li>
<li class="chapter" data-level="1.6" data-path="simple-regression-model.html"><a href="simple-regression-model.html#hypothesis-testing-and-confidence-interval-estimation"><i class="fa fa-check"></i><b>1.6</b> Hypothesis testing and confidence interval estimation</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="simple-regression-model.html"><a href="simple-regression-model.html#confidence-interval-for-regression-coefficients"><i class="fa fa-check"></i><b>1.6.1</b> Confidence interval for regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="simple-regression-model.html"><a href="simple-regression-model.html#problems"><i class="fa fa-check"></i>Problems</a></li>
<li class="chapter" data-level="" data-path="simple-regression-model.html"><a href="simple-regression-model.html#solutions"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html"><i class="fa fa-check"></i><b>2</b> Multiple regression model</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html#multiple-regression-model-1"><i class="fa fa-check"></i><b>2.1</b> Multiple Regression Model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html#goodness-of-fit-redux"><i class="fa fa-check"></i><b>2.2</b> Goodness of fit redux</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html#hypothesis-testing-in-a-multiple-regression-model"><i class="fa fa-check"></i><b>2.3</b> Hypothesis testing in a multiple regression model</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html#test-of-statistical-significance-of-the-entire-model"><i class="fa fa-check"></i><b>2.3.1</b> Test of statistical significance of the entire model</a></li>
<li class="chapter" data-level="2.3.2" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html#testing-whether-one-or-more-of-the-variables-can-be-eliminated-from-the-model"><i class="fa fa-check"></i><b>2.3.2</b> Testing whether one or more of the variables can be eliminated from the model</a></li>
<li class="chapter" data-level="2.3.3" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html#testing-a-linear-restriction-on-slope-coefficients"><i class="fa fa-check"></i><b>2.3.3</b> Testing a linear restriction on slope coefficients</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html#problems-1"><i class="fa fa-check"></i>Problems</a></li>
<li class="chapter" data-level="" data-path="multiple-regression-model.html"><a href="multiple-regression-model.html#solutions-1"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="functional-form-and-dummy-variables.html"><a href="functional-form-and-dummy-variables.html"><i class="fa fa-check"></i><b>3</b> Functional form and dummy variables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="functional-form-and-dummy-variables.html"><a href="functional-form-and-dummy-variables.html#polynomials-in-the-regression-model"><i class="fa fa-check"></i><b>3.1</b> Polynomials in the regression model</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="functional-form-and-dummy-variables.html"><a href="functional-form-and-dummy-variables.html#obtaining-optimal-polynomial-order-using-goodness-of-fit"><i class="fa fa-check"></i><b>3.1.1</b> Obtaining optimal polynomial order using goodness of fit</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="functional-form-and-dummy-variables.html"><a href="functional-form-and-dummy-variables.html#step-functions-dummy-variables-in-the-regression-model"><i class="fa fa-check"></i><b>3.2</b> Step functions: Dummy variables in the regression model</a></li>
<li class="chapter" data-level="" data-path="functional-form-and-dummy-variables.html"><a href="functional-form-and-dummy-variables.html#problems-2"><i class="fa fa-check"></i>Problems</a></li>
<li class="chapter" data-level="" data-path="functional-form-and-dummy-variables.html"><a href="functional-form-and-dummy-variables.html#solutions-2"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html"><i class="fa fa-check"></i><b>4</b> Classical assumptions and OLS estimator</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#classical-assumptions"><i class="fa fa-check"></i><b>4.1</b> Classical Assumptions</a></li>
<li class="chapter" data-level="4.2" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#heteroscedasticity"><i class="fa fa-check"></i><b>4.2</b> Heteroscedasticity</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#consequences-of-heteroscedasticity-for-the-ols-estimator"><i class="fa fa-check"></i><b>4.2.1</b> Consequences of Heteroscedasticity for the OLS estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#testing-for-hetroscedasticity-in-data"><i class="fa fa-check"></i><b>4.3</b> Testing for Hetroscedasticity in data</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#lm-test-for-linear-heteroscedasticity-bp-test"><i class="fa fa-check"></i><b>4.3.1</b> LM test for linear heteroscedasticity: BP test</a></li>
<li class="chapter" data-level="4.3.2" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#lm-test-for-linear-heteroscedasticity-whites-test"><i class="fa fa-check"></i><b>4.3.2</b> LM test for linear heteroscedasticity: Whiteâ€™s test</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#heteroscedasticity-robust-standard-errors"><i class="fa fa-check"></i><b>4.4</b> Heteroscedasticity robust standard errors</a></li>
<li class="chapter" data-level="4.5" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#serial-correlation"><i class="fa fa-check"></i><b>4.5</b> Serial correlation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#consequences-of-serial-correlation-for-the-ols-estimator"><i class="fa fa-check"></i><b>4.5.1</b> Consequences of Serial Correlation for the OLS estimator</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#testing-for-serial-correlation-in-data"><i class="fa fa-check"></i><b>4.6</b> Testing for Serial Correlation in data</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#durbin-watson-test-for-first-order-serial-correlation"><i class="fa fa-check"></i><b>4.6.1</b> Durbin-Watson test for first order serial correlation</a></li>
<li class="chapter" data-level="4.6.2" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#breusch-godfrey-bg-test-for-serial-correlation"><i class="fa fa-check"></i><b>4.6.2</b> Breusch-Godfrey (BG) test for serial correlation</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#serial-correlation-robust-standard-errors"><i class="fa fa-check"></i><b>4.7</b> Serial correlation robust standard errors</a></li>
<li class="chapter" data-level="" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#problems-3"><i class="fa fa-check"></i>Problems</a></li>
<li class="chapter" data-level="" data-path="classical-assumptions-and-ols-estimator.html"><a href="classical-assumptions-and-ols-estimator.html#solutions-3"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html"><i class="fa fa-check"></i><b>5</b> Instrumental Variable Estimation</a>
<ul>
<li class="chapter" data-level="5.1" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#endogeneity-problem"><i class="fa fa-check"></i><b>5.1</b> Endogeneity Problem</a></li>
<li class="chapter" data-level="5.2" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#omitted-variable-bias"><i class="fa fa-check"></i><b>5.2</b> Omitted variable bias</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#consequence-of-omitted-variable-on-ols-estimator"><i class="fa fa-check"></i><b>5.2.1</b> Consequence of omitted variable on OLS estimator</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#iv-estimation"><i class="fa fa-check"></i><b>5.3</b> IV estimation</a></li>
<li class="chapter" data-level="5.4" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#case-i-one-endogenous-regressor-and-one-instrument"><i class="fa fa-check"></i><b>5.4</b> Case I: One endogenous regressor and one instrument</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#example-1-return-to-schooling"><i class="fa fa-check"></i><b>5.4.1</b> Example 1: Return to Schooling</a></li>
<li class="chapter" data-level="5.4.2" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#example-2-estimating-demand-for-butter"><i class="fa fa-check"></i><b>5.4.2</b> Example 2: Estimating Demand for Butter</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#case-ii-one-endogenous-regressor-with-many-available-instruments"><i class="fa fa-check"></i><b>5.5</b> Case II: One endogenous regressor with many available instruments</a></li>
<li class="chapter" data-level="5.6" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#iv-estimation-in-a-multiple-regression-framework"><i class="fa fa-check"></i><b>5.6</b> IV estimation in a multiple regression framework</a></li>
<li class="chapter" data-level="5.7" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#strength-and-exogneity-of-the-instrument"><i class="fa fa-check"></i><b>5.7</b> Strength and Exogneity of the Instrument</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#weak-instruments"><i class="fa fa-check"></i><b>5.7.1</b> Weak Instruments</a></li>
<li class="chapter" data-level="5.7.2" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#exogeneity-of-instruments"><i class="fa fa-check"></i><b>5.7.2</b> Exogeneity of Instruments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#problems-4"><i class="fa fa-check"></i>Problems</a></li>
<li class="chapter" data-level="" data-path="instrumental-variable-estimation.html"><a href="instrumental-variable-estimation.html#solutions-4"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="discrete-choice-model.html"><a href="discrete-choice-model.html"><i class="fa fa-check"></i><b>6</b> Discrete Choice Model</a>
<ul>
<li class="chapter" data-level="6.1" data-path="discrete-choice-model.html"><a href="discrete-choice-model.html#binary-dependent-variable"><i class="fa fa-check"></i><b>6.1</b> Binary Dependent Variable</a></li>
<li class="chapter" data-level="" data-path="discrete-choice-model.html"><a href="discrete-choice-model.html#problems-5"><i class="fa fa-check"></i>Problems</a></li>
<li class="chapter" data-level="" data-path="discrete-choice-model.html"><a href="discrete-choice-model.html#solutions-5"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="review-of-differential-calculus-and-optimization.html"><a href="review-of-differential-calculus-and-optimization.html"><i class="fa fa-check"></i><b>A</b> Review of Differential Calculus and Optimization</a>
<ul>
<li class="chapter" data-level="A.1" data-path="review-of-differential-calculus-and-optimization.html"><a href="review-of-differential-calculus-and-optimization.html#derivative-of-a-single-variable-function"><i class="fa fa-check"></i><b>A.1</b> Derivative of a single variable function</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="review-of-differential-calculus-and-optimization.html"><a href="review-of-differential-calculus-and-optimization.html#rules-of-differentiation"><i class="fa fa-check"></i><b>A.1.1</b> Rules of Differentiation</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="review-of-differential-calculus-and-optimization.html"><a href="review-of-differential-calculus-and-optimization.html#second-derivative-and-non-linearity"><i class="fa fa-check"></i><b>A.2</b> Second derivative and non-linearity</a></li>
<li class="chapter" data-level="A.3" data-path="review-of-differential-calculus-and-optimization.html"><a href="review-of-differential-calculus-and-optimization.html#partial-derivatives-multi-variable-functions"><i class="fa fa-check"></i><b>A.3</b> Partial derivatives: Multi-variable functions</a></li>
<li class="chapter" data-level="A.4" data-path="review-of-differential-calculus-and-optimization.html"><a href="review-of-differential-calculus-and-optimization.html#optimization"><i class="fa fa-check"></i><b>A.4</b> Optimization</a></li>
<li class="chapter" data-level="" data-path="review-of-differential-calculus-and-optimization.html"><a href="review-of-differential-calculus-and-optimization.html#problems-6"><i class="fa fa-check"></i>Problems</a></li>
<li class="chapter" data-level="" data-path="review-of-differential-calculus-and-optimization.html"><a href="review-of-differential-calculus-and-optimization.html#solutions-6"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html"><i class="fa fa-check"></i><b>B</b> Review of Probability and Statistics</a>
<ul>
<li class="chapter" data-level="B.1" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#probability"><i class="fa fa-check"></i><b>B.1</b> Probability</a></li>
<li class="chapter" data-level="B.2" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#random-variable"><i class="fa fa-check"></i><b>B.2</b> Random Variable</a></li>
<li class="chapter" data-level="B.3" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#probability-distribution"><i class="fa fa-check"></i><b>B.3</b> Probability distribution</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#probability-distribution-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>B.3.1</b> Probability distribution of a discrete random variable</a></li>
<li class="chapter" data-level="B.3.2" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#probability-distribution-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>B.3.2</b> Probability distribution of a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="B.4" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#moments-of-a-probability-distribution-function"><i class="fa fa-check"></i><b>B.4</b> Moments of a probability distribution function</a>
<ul>
<li class="chapter" data-level="B.4.1" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#first-moment-of-a-probability-distribution-expected-value"><i class="fa fa-check"></i><b>B.4.1</b> First moment of a probability distribution: Expected value</a></li>
<li class="chapter" data-level="B.4.2" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#second-moment-of-the-distribution."><i class="fa fa-check"></i><b>B.4.2</b> Second moment of the distribution.</a></li>
<li class="chapter" data-level="B.4.3" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#third-and-fourth-moments-skewness-and-kurtosis"><i class="fa fa-check"></i><b>B.4.3</b> Third and Fourth Moments: Skewness and Kurtosis</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#useful-probability-distributions"><i class="fa fa-check"></i><b>B.5</b> Useful probability distributions</a></li>
<li class="chapter" data-level="B.6" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#joint-probability-distribution"><i class="fa fa-check"></i><b>B.6</b> Joint Probability Distribution</a></li>
<li class="chapter" data-level="B.7" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#measures-of-statistical-association"><i class="fa fa-check"></i><b>B.7</b> Measures of statistical association</a>
<ul>
<li class="chapter" data-level="B.7.1" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#rules-of-expectation-and-variances"><i class="fa fa-check"></i><b>B.7.1</b> Rules of expectation and variances</a></li>
</ul></li>
<li class="chapter" data-level="B.8" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#sampling-and-estimation"><i class="fa fa-check"></i><b>B.8</b> Sampling and Estimation</a></li>
<li class="chapter" data-level="B.9" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#hypothesis-testing"><i class="fa fa-check"></i><b>B.9</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="B.9.1" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#testing-a-restriction-on-a-single-population-parameter"><i class="fa fa-check"></i><b>B.9.1</b> Testing a restriction on a single population parameter</a></li>
<li class="chapter" data-level="B.9.2" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#testing-a-restriction-on-multiple-population-parameter"><i class="fa fa-check"></i><b>B.9.2</b> Testing a restriction on multiple population parameter</a></li>
<li class="chapter" data-level="B.9.3" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#confidence-interval-and-hypothesis-testing"><i class="fa fa-check"></i><b>B.9.3</b> Confidence interval and Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#problems-7"><i class="fa fa-check"></i>Problems</a></li>
<li class="chapter" data-level="" data-path="review-of-probability-and-statistics.html"><a href="review-of-probability-and-statistics.html#solutions-7"><i class="fa fa-check"></i>Solutions</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="statistical-tables.html"><a href="statistical-tables.html"><i class="fa fa-check"></i><b>C</b> Statistical Tables</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-tables.html"><a href="statistical-tables.html#table-a-critical-values-for-the-t-distribution"><i class="fa fa-check"></i>Table A: Critical Values for the t-distribution</a></li>
<li class="chapter" data-level="" data-path="statistical-tables.html"><a href="statistical-tables.html#table-b-critical-values-for-the-chi-square-distribution"><i class="fa fa-check"></i>Table B: Critical Values for the Chi-square distribution</a></li>
<li class="chapter" data-level="" data-path="statistical-tables.html"><a href="statistical-tables.html#table-c-1-critical-values-for-the-f-distribution"><i class="fa fa-check"></i>Table C: 1% Critical Values for the F distribution</a></li>
<li class="chapter" data-level="" data-path="statistical-tables.html"><a href="statistical-tables.html#table-d-5-critical-values-for-the-f-distribution"><i class="fa fa-check"></i>Table D: 5% Critical Values for the F distribution</a></li>
<li class="chapter" data-level="" data-path="statistical-tables.html"><a href="statistical-tables.html#table-e-10-critical-values-for-the-f-distribution"><i class="fa fa-check"></i>Table E: 10% Critical Values for the F distribution</a></li>
<li class="chapter" data-level="" data-path="statistical-tables.html"><a href="statistical-tables.html#table-f-5-one-sided-critical-values-for-the-durbin-watson-distribution"><i class="fa fa-check"></i>Table F: 5% One-sided Critical Values for the Durbin-Watson Distribution</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://vipul-bhatt.github.io/Econ-385-Notes/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Econometrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-regression-model" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">Chapter 1</span> Simple Regression Model<a href="simple-regression-model.html#simple-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter we consider the simple case of establishing an empirical relationship between two variables. For simplicity, we will denote the outcome variable (or dependent variable) by <span class="math inline">\(Y\)</span> and the explanatory variable (or independent variable) by <span class="math inline">\(X\)</span>. Throughout this chapter we will ignore the limitations of this model and assume that all the assumptions needed for this kind of model are met in our data. In subsequent chapters we will learn how this model can be extended to incorporate real world issues we face when conducting quantitative economic analysis.</p>
<div id="statistical-foundation-of-the-simple-regression-model" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Statistical foundation of the simple regression model<a href="simple-regression-model.html#statistical-foundation-of-the-simple-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A simple economic relationship often requires us to specify the population of interest and variables from this population that we wish to empirically examine. For example, we may be interested in finding out how college major choice affects starting salary of graduates. Here, the population of interest is college graduates and the variables representing this population are starting salary and college major. In this example, the outcome variable is starting salary (denoted by Y) and the explanatory variable is college major choice (X). A simple regression model addresses the following question: how does changes in <span class="math inline">\(X\)</span> affect <span class="math inline">\(Y\)</span>? In our example, how does differences in college majors between individuals relate to differences in their starting salaries. This is the conditional expected value of Y given X, denoted by <span class="math inline">\(E(Y_i|X_i)\)</span> and is known as the <strong>population regression function (PRF)</strong>. Because we usually work with a sample, the population regression function is unknown to us and our goal is to estimate this conditional mean using a sample data on Y and X.</p>
<p>Before attempting to estimate the PRF, we have to first make an assumption about the functional form for this function. For example, we can assume that:</p>
<p><span class="math display">\[E(Y_i|X_i)=\beta_0 + \beta_1 X_i\]</span></p>
<p>In this case we are assuming that changes in <span class="math inline">\(X\)</span> affects expected value of <span class="math inline">\(Y\)</span> in a linear fashion (constant slope, given by <span class="math inline">\(\beta_1\)</span>). Note that we can accommodate any kind of functional relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> by simply changing the functional form. For example, a quadratic specification is given by:</p>
<p><span class="math display">\[E(Y_i|X_i)=\beta_0 + \beta_1 X_i^2\]</span></p>
<p>Here, we assume that changes in X affect Y in a non-linear fashion. In this sense, the simple linear regression model is flexible enough to capture the relationship between X and Y. However, this also means that getting the functional form wrong will introduce a <strong>specification error</strong> in our estimation. Depending on whether we want <strong>explain</strong> Y using data on X or <strong>predict</strong> Y using data on X, the consequences of the specification error could be less or more severe. Here it is also important to emphasize the difference between <strong>linear in parameters</strong> vs <strong>linear in variables</strong>. Both of the assumed functional forms are linear in parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. However, the first specification is linear in X whereas the second specification is non-linear in X. In this course, we will only focus on specifications that are linear in parameters.</p>
<p>Once we have assumed a functional form for the PRF, the next step is to estimate the model parameters of interest: intercept (<span class="math inline">\(\beta_0\)</span>) and slope (<span class="math inline">\(\beta_1\)</span>). These are unknown parameters that characterize the relationship between Y and X. If we knew their values, then using data on X we can compute the model value of Y. For example, suppose <span class="math inline">\(\beta_0=5\)</span> and <span class="math inline">\(\beta_1\)</span>=2. Then, if X=10, we get:</p>
<p><span class="math display">\[E(Y_i|X_i=10)=5+2\times 10=25\]</span></p>
<p>Note that the above value of Y is what our model expects Y to be when X takes a value of 10. In data, we may find that the actual value of Y corresponding to X=10 is different. This difference between data on Y and model-generated value of Y is called the <strong>regression error term</strong> and is denoted by <span class="math inline">\(\epsilon_i\)</span>. Formally,</p>
<p><span class="math display">\[\epsilon_i= Y_i - E(Y_i|X_i)\]</span></p>
<p>or equivalently,</p>
<p><span class="math display">\[Y_i = E(Y_i|X_i) + \epsilon_i\]</span></p>
<p>By substituting our assumed functional form for <span class="math inline">\(E(Y_i|X_i)\)</span> in the above equation we obtain the simple regression model. For example, if we assume that <span class="math inline">\(E(Y_i|X_i)=\beta_0 + \beta_1 X_i\)</span>, then the corresponding regression model is given by:</p>
<p><span class="math display" id="eq:simple" id="eq:simple">\[\begin{equation}
Y_i=\beta_0 + \beta_1 X_i + \epsilon_i
\tag{1.1}
\end{equation}\]</span></p>
<p>The specification in \tag{1.1} is the simple regression model with parameter <span class="math inline">\(\beta_1\)</span> capturing the effect of changes in X on Y:</p>
<p><span class="math display">\[\beta_1= \frac{dY_i}{dX_i}\]</span></p>
<p>So if X changes by 1 unit, then Y changes by <span class="math inline">\(\beta_1\)</span> units. Under certain assumptions that we will discuss later in the course, <span class="math inline">\(\beta_1\)</span> is also the causal effect of X on Y.</p>
</div>
<div id="ordinary-least-squares-ols" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Ordinary Least Squares (OLS)<a href="simple-regression-model.html#ordinary-least-squares-ols" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The goal of any empirical analysis is to find a <strong>functional</strong> relationship between the outcome (or dependent variable) and the explanatory (or control variable). As discussed in the previous section, the PRF is one such function. Once we have estimated this function it can be used for either explaining variations in outcome caused by variations in control or to predict the outcome given data on control. For example, economic theory tells us that consumption and income are positively related implying there must be a numerical mapping between these two variables. For example, assuming a linear relationship between consumption (Y) and income (X) we get the following PRF:</p>
<p><span class="math display">\[E(Y_i|X_i)=\beta_0 +\beta_1 X_i\]</span></p>
<p>As discussed earlier, this relationship will not be perfect in data. This could be due to other factors affecting consumption such as gifts, inheritances, wealth etc. The regression error term captures all such influences on consumption that stem from non-income sources. This error can be negative or positive and is captured in the following regression model:</p>
<p><span class="math display">\[Y_i=E(Y_i|X_i)+\epsilon_i \]</span></p>
<p>Or equivalently,</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1 X_i + \epsilon_i\]</span></p>
<p>If we knew the error term for each observation, then given data on consumption and income, we can simply choose values of the intercept and slope that best fits our data. However, by definition the regression error is a random variable whose values we do not observe. Consequently, the two parameters of the model, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are also unknown to us. To circumvent this problem, we focus on the sample counterpart of the regression error term called the <strong>residuals</strong> which is denoted by <span class="math inline">\(e_i\)</span> and can be computed using data on the outcome and the control variable. Let <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> be the sample estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. Let <span class="math inline">\(\hat{Y_i}\)</span> denote the <strong>predicted</strong> value of consumption given by:</p>
<p><span class="math display">\[\hat{Y_i}=\hat{\beta_0} + \hat{\beta_1} X_i \]</span></p>
<p>The predicted value of consumption is also the sample estimator of <span class="math inline">\(E(Y_i|X_i)\)</span>. Now, the residuals are defined as the difference between actual consumption and consumption predicted from our model:</p>
<p><span class="math display">\[e_i=Y_i -\hat{Y_i}= Y_i- \hat{\beta_0} - \hat{\beta_1} X_i\]</span></p>
<p>Given data on consumption and income, we can now choose values of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> that will minimize our residuals in some sense. This is displayed below in Figure <a href="simple-regression-model.html#fig:ch3fig1">1.1</a> below:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch3fig1"></span>
<img src="bookdown-demo_files/figure-html/ch3fig1-1.png" alt="Line of best-fit" width="672" />
<p class="caption">
Figure 1.1: Line of best-fit
</p>
</div>
<p>In Figure <a href="simple-regression-model.html#fig:ch3fig1">1.1</a>, each red dot denote the level of income and corresponding level of consumption for that particular observation. The blue solid line is our estimated regression line or <span class="math inline">\(\hat{Y_i}\)</span>. The gap between the red dot and the regression line is the residual for that observation. In this graph we hope that the regression line passes through our data as closely as possible. In other words, we would like our residuals to be as small as possible.</p>
<p>The canonical estimation method used in Econometrics for finding the line of best fit is called the <strong>Ordinary Least Squares (OLS)</strong>. Here, the values of <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are chosen such that the sum of squared residuals is minimized. Let RSS denotes sum of squared residuals which is given by:</p>
<p><span class="math display">\[RSS=\sum_{i=1}^{N} e_i^2=\sum_{i=1}^{N}(Y_i- \hat{\beta_0} - \hat{\beta_1} X_i)^2\]</span></p>
<p>Mathematically, OLS solves the following optimization problem:</p>
<p><span class="math display">\[min_{\hat{\beta_0}, \hat{\beta_1}} \quad \sum_{i=1}^N RSS\]</span></p>
<p>The solution to the above optimization problem gives us the following formulas:</p>
<p><span class="math display">\[\hat{\beta_0}= \overline{Y} - \hat{\beta_1}\overline{X}\]</span></p>
<p>and</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{\sum_{i=1}^N (Y_i-\overline{Y})(X_i-\overline{X})}{\sum_{i=1}^N(X_i-\overline{X})^2}\]</span></p>
<p>where <span class="math inline">\(\overline{Y}\)</span> and <span class="math inline">\(\overline{X}\)</span> denote sample means of consumption (Y) and income (X). By definition, these OLS estimators satisfy the following desirable properties:</p>
<ol style="list-style-type: decimal">
<li><p>Sum of residuals is zero, i.e., <span class="math inline">\(\sum_{i=1}^N e_i=0\)</span>.</p></li>
<li><p>Residuals and the explanatory variables are uncorrelated, i.e, <span class="math inline">\(\sum_{i=1}^N e_iX_i=0\)</span>.</p></li>
</ol>
<div class="example">
<p><span id="exm:unnamed-chunk-2" class="example"><strong>Example 1.1  </strong></span>Suppose you have the following data:</p>
<caption>
<span id="tab:ch3table1">Table 1.1: </span> OLS Estimation Example
</caption>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(ID\)</span></th>
<th align="center"><span class="math inline">\(Y_i\)</span></th>
<th align="center"><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">10</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">8</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">12</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">14</td>
<td align="center">8</td>
</tr>
</tbody>
</table>
<p>Suppose we assume that the relationship between Y and X can be captured by the following regression model:</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1 X_i + \epsilon_i\]</span></p>
<p>We can now use the data given to us and compute the OLS estimators for two regression parameters. First, note that the sample means are given by: <span class="math inline">\(\overline{Y}=11\)</span> and <span class="math inline">\(\overline{X}=5\)</span>. Second, we compute the slope cofficient by utilizing the information provided in the table below:</p>
<caption>
<span id="tab:ch3table2">Table 1.2: </span> OLS Estimation Example contd..
</caption>
<table>
<colgroup>
<col width="8%" />
<col width="4%" />
<col width="3%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(ID\)</span></th>
<th align="center"><span class="math inline">\(Y_i\)</span></th>
<th align="center"><span class="math inline">\(X_i\)</span></th>
<th align="center"><span class="math inline">\(Y_i-\overline{Y}\)</span></th>
<th align="center"><span class="math inline">\(X_i-\overline{X}\)</span></th>
<th align="center"><span class="math inline">\((Y_i-\overline{Y})(X_i-\overline{X})\)</span></th>
<th align="center"><span class="math inline">\((X_i-\overline{X})^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">10</td>
<td align="center">3</td>
<td align="center">-1</td>
<td align="center">-2</td>
<td align="center">2</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">8</td>
<td align="center">6</td>
<td align="center">-3</td>
<td align="center">1</td>
<td align="center">-3</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">12</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">-2</td>
<td align="center">-2</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">14</td>
<td align="center">8</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">9</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="center"><strong>Total</strong></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">6</td>
<td align="center">18</td>
</tr>
</tbody>
</table>
<p>Using the OLS formular, we get:</p>
<p><span class="math display">\[\hat{\beta_1}=\frac{6}{18}=0.33\]</span></p>
<p><span class="math display">\[\hat{\beta_0}= 11-.33\times 5= 9.33\]</span></p>
<p>Hence, our model provides us the following equation for the predicted value of <span class="math inline">\(Y_i\)</span>:</p>
<p><span class="math display">\[\hat{Y_i} = 9.33 + 0.33 \times X_i \]</span></p>
<p>Using the above equation we can easily find what our model predicts about the dependent variable given data on X. Note that we can compute this predicted value for observations for which we have data on X and compare it with the actual data on <span class="math inline">\(Y\)</span>. The difference between the two gives us the regression residuals. This is provided in the table below:</p>
<caption>
<span id="tab:ch3table3">Table 1.3: </span> OLS Estimation Example contd..
</caption>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(ID\)</span></th>
<th align="center"><span class="math inline">\(Y_i\)</span></th>
<th align="center"><span class="math inline">\(X_i\)</span></th>
<th align="center"><span class="math inline">\(\hat{Y_i}\)</span></th>
<th align="center"><span class="math inline">\(e_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">10</td>
<td align="center">3</td>
<td align="center">10.32</td>
<td align="center">-0.32</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">8</td>
<td align="center">6</td>
<td align="center">11.31</td>
<td align="center">-3.31</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">12</td>
<td align="center">3</td>
<td align="center">10.32</td>
<td align="center">1.68</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">14</td>
<td align="center">8</td>
<td align="center">11.97</td>
<td align="center">2.03</td>
</tr>
</tbody>
</table>
<p>Using data used in estimating the regression model to compute predicted values of the outcome variable is called <strong>in-sample</strong> prediction. In the above table, we have in-sample prediction for the outcome variable. We can also similarly compute the predicted value for any value of <span class="math inline">\(X\)</span> even outside our given sample. Such an exercise is called <strong>out-sample</strong> prediction. So for example, consider <span class="math inline">\(X=7\)</span> which is not part of our sample. We can compute the corresponding predicted value of <span class="math inline">\(Y\)</span> is given by:</p>
<p><span class="math display">\[\hat{Y} = 9.33+0.33\times 7= 11.64 \]</span></p>
<p>This is one of the primary uses of a regression model, namely, the ability to predict the value of the outcome variable, given information on the value of the control variable. Another objective of estimating a regression model is to explain how <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span>. In our example, a 1 unit change in <span class="math inline">\(X\)</span> will change <span class="math inline">\(Y\)</span> by 0.33 units. Under some assumptions, this denotes the causal effect of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The sign and the size of this effect provides meaningful information on the nature of the relationship between the outcome and the control variables.</p>
</div>
</div>
<div id="goodness-of-fit" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Goodness of fit<a href="simple-regression-model.html#goodness-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After estimating a regression model using OLS a natural question to ask is how good is our estimated model in fitting data on the outcome variable. One measure of this is the <strong>goodness of fit</strong> which is based on the proportion of total variation in the dependent variable that can be explained by our model. A better fit would imply greater amount of variation being explained.</p>
<p>Essentially, our goal is to decompose the total variation in the dependent variable into two components, nameley, <strong>explained variation</strong> and <strong>residual variation</strong>. Formally,<br />
rewriting the definition of residuals to get the following relationship:</p>
<p><span class="math display">\[Y_i = \hat{Y_i}+ e_i\]</span></p>
<p>Subtract mean of the dependent variable from both sides we get:</p>
<p><span class="math display">\[Y_i-\overline{Y} = \hat{Y_i}-\overline{Y}+ e_i\]</span></p>
<p>Finally, squaring and summing across all observations gives us the following identity:</p>
<p><span class="math display">\[\underbrace{\sum_{i=1}^N (Y_i-\overline{Y})^2}_{TSS}=\underbrace{\sum_{i=1}^N (\hat{Y_i}-\overline{Y})^2}_{ESS}+\underbrace{\sum_{i=1}^N e_i^2}_{RSS}\]</span></p>
<p>where <span class="math inline">\(TSS\)</span> stands for total sum of squares and measures variation in the dependent variable, <span class="math inline">\(Y_i\)</span>. <span class="math inline">\(ESS\)</span> denotes explained sum of squares and measures variation in the predicted value of the dependent variable, <span class="math inline">\(\hat{Y_i}\)</span>. Finally, <span class="math inline">\(RSS\)</span>, as previously defined captures variation in regression residuals. We can now define a measure of fit known as the <strong>R-squared</strong> which is denoted by <span class="math inline">\(R^2\)</span> and is defined as the ratio of <span class="math inline">\(ESS\)</span> to <span class="math inline">\(TSS\)</span>. Formally,</p>
<p><span class="math display">\[R^2=\frac{ESS}{TSS}\]</span></p>
<p>Hence, <span class="math inline">\(R^2\)</span> tells us the fraction of total variation in the dependent variable that can be explained by our model. If <span class="math inline">\(R^2=0\)</span>, then our model explains none of the variation in <span class="math inline">\(Y_i\)</span>. Similarly, <span class="math inline">\(R^2=1\)</span>, then our model explains all of the variation in <span class="math inline">\(Y_i\)</span>. As a result, <span class="math inline">\(R^2\)</span> will always be a number between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> with a higher number indicating a better fit. Note that by definition:</p>
<p><span class="math display">\[1-R^2= \frac{RSS}{TSS}\]</span></p>
<p>As a result, <span class="math inline">\(1-R^2\)</span> gives us the unexplained variation. For example, a value of 0.4 will imply 40% of the variation in <span class="math inline">\(Y_i\)</span> is explained by our model or 60% of the variation is not explained.</p>
</div>
<div id="applications-of-the-simple-regression-model" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Applications of the simple regression model<a href="simple-regression-model.html#applications-of-the-simple-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="aggregate-consumption-function" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Aggregate consumption function<a href="simple-regression-model.html#aggregate-consumption-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose you are interested in finding out the relationship between consumption and income at the national level. The first step in any empirical anaylsis is to determine the empirical measures of the variables of interest, consumption and disposable income here. We will use real personal consumption expenditure as a measure of <span class="math inline">\(C\)</span> and real personal disposable income as a measure of <span class="math inline">\(Y^D\)</span>. Both of these data are available at the monthly frequency from the FRED Stat database and can be downloaded from the following links:</p>
<ol style="list-style-type: lower-roman">
<li><p><a href="https://fred.stlouisfed.org/series/PCEC96" class="uri">https://fred.stlouisfed.org/series/PCEC96</a></p></li>
<li><p><a href="https://fred.stlouisfed.org/series/DSPIC96" class="uri">https://fred.stlouisfed.org/series/DSPIC96</a></p></li>
</ol>
<p>We will use the data from Jan-2002 through June 2019. The next step is to formulate our regression model. Here, we can use economic theory and propose a linear relationship between consumption and income:</p>
<p><span class="math display">\[E(Consumption_t|Income_t)=\beta_0+\beta_1 Income_t\]</span></p>

<div class="rmdNote">
<p>In macroeconomics, the above relationship is known as the <strong>Keynesian</strong> consumption function. John Maynard Keynes proposed that at the aggregate level, consumption changes in proportion to changes in disposable income:</p>
<span class="math display">\[C = a + b \ Y^D\]</span> Here <span class="math inline">\(C\)</span> denotes private consumption expenditure and <span class="math inline">\(Y^D\)</span> denotes post-tax or disposable income. <span class="math inline">\(a\)</span> is the intercept and captures the part of consumption that is indpenedent of income. <span class="math inline">\(b\)</span> is the slope and measures the unit change in consumption caused by a unit change in disposable income. <span class="math inline">\(b\)</span> measures the marginal propensity to consume and is a parameter of interest we would like to estimate using data.
</div>
<p>The implied regression model is given by:</p>
<p><span class="math display">\[Consumption_i  = \beta_0 + \beta_1 Income_i + \epsilon_i\]</span></p>
<p>Our parameter of interest is <span class="math inline">\(\beta_1\)</span> which maps to the marginal propensity of consume in the original regression model.
Table <a href="simple-regression-model.html#tab:ch3table4">1.4</a> below presents a summary of the OLS estimation results for the above model. A coefficient of 0.82 for income implies that for every dollar increase in disposable income, consumption increases by 82 cents. Under certain assumptions which will be discussed in Chapter 5, this is the <strong>causal</strong> effect of change in income on consumption at the aggregate level. Note that this relationship is estimated using time series data and does not tell us anything about how changes in income across individuals affect their consumption spending. We also note that the R-squared is 0.986 which means that roughly 98 percent of variation in the aggregate consumption expenditure from one month to another can be explained by personal disposable income.</p>
<caption>
<span id="tab:ch3table4">Table 1.4: </span> OLS Estimation of Keynesian Consumption Function
</caption>
<table style="border: solid 2px;">
<tr>
<th style="font-weight: bold; border-top: solid 2px; text-align:left; ">
Â 
</th>
<th colspan="4" style="font-weight: bold; border-top: solid 2px;">
Dependent variable: Real Personal Consumption Expenditure
</th>
</tr>
<tr>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align:left; ">
Explanatory Variables
</td>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align: center;">
b
</td>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align:center;">
s.e.(b)
</td>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align: center;">
t-ratio
</td>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align: center;">
p-value
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Intercept
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
1205.284 <sup>***</sup>
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align:center;">
82.638
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
14.585
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Real Personal Disposable Income
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
0.805 <sup>***</sup>
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align:center;">
0.007
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
118.742
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:2px solid;">
Observations
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:center; border-top:2px solid;" colspan="4">
210
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:center;" colspan="4">
0.985 / 0.985
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
log-Likelihood
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:center;" colspan="4">
-1316.664
</td>
</tr>
<tr>
<td colspan="5" style="font-style:italic; border-top:double black; text-align:right;">
<ul>
<li>p&lt;0.05Â Â Â ** p&lt;0.01Â Â Â *** p&lt;0.001
</td>
</tr></li>
</ul>
</table>
<p>Note that we can use our estimated regression model to predict future aggregate consumption expenditure. Our regression used data from Jan 2002 through June 2019. Suppose, we believe that in Jan 2020, the real personable disposable income will be 1% higher than the June 2019 level of $15007.5 Billion. Then, using our estimated model, the real personal consumption expenditure will be:</p>
<p><span class="math display">\[Consumption_{Jan, 2020} = 1037.389+ 0.82*(15007.5 \times 1.01)=\$13475.29 \ Billion\]</span></p>
<p>Note that instead of assuming a single value for the future income, we can consider a range of scenarios and compute the predicted consumption expenditure for each scenario. These scenarios are based on our understanding of the state of the economy and policy environment. For example, what if there is a tax cut that increases disposable income? Or what if there is a crash in the housing market leading to a large drop in disposable income for many households? This type of exercise is known as <strong>scenario-based</strong> forecasting. Table <a href="simple-regression-model.html#tab:ch3table5">1.5</a> below provides future consumption expenditure for these 3 scenarios.</p>
<caption>
<span id="tab:ch3table5">Table 1.5: </span> OLS Estimation of Keynesian Consumption Function
</caption>
<table>
<colgroup>
<col width="38%" />
<col width="32%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Scenario of Jan 2020</th>
<th align="center">Real Personal Disposable Income</th>
<th align="center">Real Consumption Expenditure</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Average income growth (Apr-Jun, 2019)</td>
<td align="center">0.19%</td>
<td align="center">$13375.61 Billions</td>
</tr>
<tr class="even">
<td align="center">One std. dev. below average</td>
<td align="center">0.07%</td>
<td align="center">$13360.84 Billions</td>
</tr>
<tr class="odd">
<td align="center">One std. dev. above average</td>
<td align="center">0.31%</td>
<td align="center">$13390.38 Billions</td>
</tr>
</tbody>
</table>
</div>
<div id="returns-to-education" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Returns to education<a href="simple-regression-model.html#returns-to-education" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An important research question in labor economics is how education affects labor income. Economic theory suggests that education makes a worker more productive through acquisition of skills that are rewarded in the labor market as higher wages. To test this in data, we would need to collect information on education and wages for a sample of workers. Often such an exercise is conducted using a cross-sectional data at a point in time. One such source is the Annual Social and Economic Supplement (ASEC) of the Current population survey (CPS) which is a monthly survey of households in the U.S. conducted by the Bureau of Labor Statistics (BLS). The data is publicly available in a user-friendly format at:</p>
<p><a href="https://cps.ipums.org/cps/" class="uri">https://cps.ipums.org/cps/</a></p>
<p>We will use a sample of 1000 observations from the ASEC 2018 in this exercise. Suppose, our theoretical model for the average relationship between income and education is log-linear:</p>
<p><span class="math display">\[E[ln(wage_i)|education_i]=\beta_0+\beta_1 education_i\]</span></p>

<div class="rmdNote">
The above formulation is a simplified version of the specification that has been extensively used in the labor economics literature. The above log-linear relationship between wages and education was made famous by Jacob Mincer and is commonly known as the <strong>Mincerian earnings function</strong>. The orginal specification also includes controls for years of experience. Using Census data from 1950 and 1960, he caluculated that every additional year of education increases annual earnings by 5 to 10%.
</div>
<p>The implied regression model is given by:</p>
<p><span class="math display">\[ln(Wage_i)= \beta_0 +\beta_1 Education_i + \epsilon_i\]</span></p>
<p>where <span class="math inline">\(Wage_i\)</span> is the annual wages and salaries of individual <span class="math inline">\(i\)</span> in dollars. <span class="math inline">\(Education_i\)</span> denotes years of education of individual <span class="math inline">\(i\)</span>. Table <a href="simple-regression-model.html#tab:ch3table6">1.6</a> below presents the estimation results. A coefficient of 0.106 for education implies that for every additional year of schooling, annual wages increase by 10.6%. Again, only under certain assumption this captures causal effect of education on wages. We also note that the R-squared is 0.122 which means that roughly 12 percent of the variation in wages across individuals can be explained by differences in their education levels.</p>
<caption>
<span id="tab:ch3table6">Table 1.6: </span> OLS Estimation of Earnings Function
</caption>
<table style="border: solid 2px;">
<tr>
<th style="font-weight: bold; border-top: solid 2px; text-align:left; ">
Â 
</th>
<th colspan="4" style="font-weight: bold; border-top: solid 2px;">
Dependent variable: Log of Annual Wages
</th>
</tr>
<tr>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align:left; ">
Explanatory Variables
</td>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align: center;">
b
</td>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align:center;">
s.e.(b)
</td>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align: center;">
t-ratio
</td>
<td style="border: solid 1px; font-weight:bold; border-top: solid 2px; text-align: center;">
p-value
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Intercept
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
9.259 <sup>***</sup>
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align:center;">
0.135
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
68.531
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; ">
Years of Schooling
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
0.106 <sup>***</sup>
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align:center;">
0.009
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
11.785
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  text-align: center;">
<strong>&lt;0.001</strong>
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:2px solid;">
Observations
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:center; border-top:2px solid;" colspan="4">
1000
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
R<sup>2</sup> / R<sup>2</sup> adjusted
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:center;" colspan="4">
0.122 / 0.121
</td>
</tr>
<tr>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;">
log-Likelihood
</td>
<td style="border: solid 1px; padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:center;" colspan="4">
-1127.689
</td>
</tr>
<tr>
<td colspan="5" style="font-style:italic; border-top:double black; text-align:right;">
<ul>
<li>p&lt;0.05Â Â Â ** p&lt;0.01Â Â Â *** p&lt;0.001
</td>
</tr></li>
</ul>
</table>

<div class="rmdNote">
<p>Notice how the interepretion of the coefficient of education is percent terms here: multiply the slope coefficient by 100 and interpret as implying a 10.6% increase in wages. This is because the dependent variable is transformed into natural logs. Mathematically, 100 times one unit change in natural logs of a variable is approximately equal to a percent change in the level of the variable. More on this in the next section.</p>
</div>
</div>
</div>
<div id="use-of-natural-logs-and-interpretation-of-the-slope-coefficients" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Use of natural logs and interpretation of the slope coefficients<a href="simple-regression-model.html#use-of-natural-logs-and-interpretation-of-the-slope-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose we have data on two variables: Y and X. Consider the following regression model:</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\]</span></p>
<p>There are two things to remember when use a model like above:</p>
<ol style="list-style-type: decimal">
<li><p>We assume that there is a linear relationship between Y and X. This is what we mean by <strong>functional form</strong> or <strong>specification</strong> of our regression model.</p></li>
<li><p>The interpretation of the slope coefficient is in terms of original units of our data. So, if <span class="math inline">\(X\)</span> denotes number of hours studied and <span class="math inline">\(Y\)</span> denotes test score in points from 1 through 100, then an additional <strong>hour</strong> of study will change your test score by <span class="math inline">\(\beta_1\)</span> <strong>points</strong>.</p></li>
</ol>
<p>Once we transform our data by using some form of mathematical operation we need to change both our belief about the functional relationship between Y and X, as well as our interpretation of the slope coefficient. One of the most commonly used transformation in economic analysis is the use of natural logs. For example, consider the following specification:</p>
<p><span class="math display">\[ln(Y_i)=\beta_0 + \beta_1 ln(X_i) + \epsilon_i\]</span></p>
<p>In this case, we believe that after transforming our data to natural logs, the relationship becomes linear. However, the above regression model implies that the relationship between Y and X in original units is non-linear. Specifically to obtain the above regression model, the underlying relationship between Y and X in original units must be:</p>
<p><span class="math display">\[e^{Y_i} = e^{\beta_0}X_i^{\beta_1}e^{\epsilon_i}\]</span></p>
<p>Further, the interpretation of <span class="math inline">\(\beta_1\)</span> is now different as well. Specifically, if ln(X) changes by 1 log point then ln(Y) changes by <span class="math inline">\(\beta_1\)</span> log points. A more intuitive interpretation is that if X changes by 1% then Y will change by <span class="math inline">\(\beta_1\)</span>%.</p>

<div class="rmdNote">
<p>To understand this section, we need to revisit the concept of natural logs. The natural logs of a number <span class="math inline">\(x\)</span> is its log using the mathematicl constant <span class="math inline">\(e\approx 2.718\)</span> as the base. The natural logs of <span class="math inline">\(x\)</span> is the power to which <span class="math inline">\(e\)</span> has to be raised to be equal to <span class="math inline">\(x\)</span>
For example, natural log of 10 is 2.302585 because <span class="math inline">\(e^{2.302585}=10\)</span>. Some useful properties of the logs that are applicable to natural logs as well are:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(ln(1)=0\)</span></p></li>
<li><p><span class="math inline">\(ln(x \times y)=ln(x)+ln(y)\)</span></p></li>
<li><p><span class="math inline">\(ln\left(\frac{x}{y}\right)=ln(x)-ln(y)\)</span></p></li>
<li><p><span class="math inline">\(ln(x^a)=a \times ln(x)\)</span></p></li>
<li><p><span class="math inline">\(ln(e^x)=e^{ln(x)}=x\)</span></p></li>
<li><p>If <span class="math inline">\(y=ln(x)\)</span>, then <span class="math inline">\(\displaystyle \frac{dy}{dx}=\frac{1}{x}\)</span></p></li>
<li><p><span class="math inline">\(\Delta ln (X) \times 100 \approx \text{percent change in X}\)</span>. This implies that <span class="math inline">\(\displaystyle \frac{d ln(Y)}{d ln(X)}=\frac{\text{percent change in Y}}{\text{percent change in X}}\)</span></p></li>
</ol>
</div>
<p>There are three types of log transformations we can use for a simple regression model. The table below shows all three with corresponding interpretations for the slope coefficient.</p>
<table>
<colgroup>
<col width="22%" />
<col width="28%" />
<col width="48%" />
</colgroup>
<thead>
<tr class="header">
<th>Model Name</th>
<th>Model Specification</th>
<th>Interpretation of <span class="math inline">\(\beta_1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Log-Log</td>
<td><span class="math display">\[ln(Y_i) = \beta_0 +\beta_1 ln(X_i)+\epsilon_i\]</span></td>
<td>If X changes by 1%, then Y changes by <span class="math inline">\(\beta_1\)</span>%</td>
</tr>
<tr class="even">
<td>Log-Linear</td>
<td><span class="math display">\[ln(Y_i) = \beta_0 +\beta_1 X_i+\epsilon_i\]</span></td>
<td>If X changes by 1 unit, then Y changes by <span class="math inline">\(100 \times \beta_1\)</span>%</td>
</tr>
<tr class="odd">
<td>Linear-Log</td>
<td><span class="math display">\[Y_i= \beta_0 +\beta_1 ln(X_i)+\epsilon_i\]</span></td>
<td>If X changes by 1%, then Y changes by <span class="math inline">\(\frac{\beta_1}{100}\)</span> units</td>
</tr>
</tbody>
</table>

<div class="rmdCaution">
If we want to compare goodness of fit of two different models, then the units of the dependent variable must be the same. In the above example, we can compare the <span class="math inline">\(R^2\)</span> of the log-log and log-linear models as they both have natural log of Y as the dependent variable. However, neither can be compared with linear-log model. This is because a linear-log model explains observed variation in the orginal units of Y. In contrast the other two models explain variations in natural logs of Y.
</div>
</div>
<div id="hypothesis-testing-and-confidence-interval-estimation" class="section level2 hasAnchor" number="1.6">
<h2><span class="header-section-number">1.6</span> Hypothesis testing and confidence interval estimation<a href="simple-regression-model.html#hypothesis-testing-and-confidence-interval-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can use the sampling distribution of the OLS estimator to carry out hypotheses about the true population parameter for interest. For example, suppose we have the following regression model:</p>
<p><span class="math display">\[Y_i =\beta_0 + \beta_1 X_i + \epsilon_i\]</span></p>
<p>Using the OLS we can obtain <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>, both of which are computed for a given random sample. As we change our sample, we get a different set of values for <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>. Hence, we should think of these OLS estimators as random variables. In principle, we can collect large number of samples and compute <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> for each sample, giving us a sampling distribution of these estimators. Let us focus on the slope parameterâ€™s OLS estimator, namely, <span class="math inline">\(\hat{\beta_1}\)</span>. Using repeated samples for Y and X, we can obtain a distribution of values for <span class="math inline">\(\hat{\beta_1}\)</span> with a mean of <span class="math inline">\(E(\hat{\beta_1})\)</span> and a variance of <span class="math inline">\(Var(\hat{\beta_1})\)</span>. Under some conditions, we can show that <span class="math inline">\(E(\hat{\beta_1})=\beta_1\)</span> and</p>
<p><span class="math display">\[Var(\hat{\beta_1})=\frac{Var(e_i)}{\sum_{i=1}^N(X_i-\overline{X})^2}\]</span></p>
<p>where <span class="math inline">\(Var(e_i)=\frac{\sum_{i=1}^N e_i^2}{N-2}\)</span> denotes the variance of the regression residuals.</p>
<p>In order to conduct hypotheses about the slope parameter, we need to make some assumption about the regression error term, <span class="math inline">\(\epsilon_i\)</span>. We will assume that the error term is normally distributed with a mean of 0 and variance of <span class="math inline">\(\sigma^2\)</span>. Then, we can test any hypothesis about <span class="math inline">\(\beta_1\)</span> using the t-test. The formal procedure for a two-sided test is outlined below:</p>
<ol style="list-style-type: decimal">
<li>Specify the null and the alternative hypotheses:</li>
</ol>
<p><span class="math display">\[H_0: \beta_1= R\]</span>
<span class="math display">\[H_0: \beta_1\neq R\]</span></p>
<p>where <span class="math inline">\(R\)</span> can be any numerical value.</p>
<ol start="2" style="list-style-type: decimal">
<li>Compute the t-ratio:</li>
</ol>
<p><span class="math display">\[t=\frac{\hat{\beta_1}-R}{s.e.(\hat{\beta_1})}\]</span></p>
<p>where <span class="math inline">\(s.e.(\hat{\beta_1})=\sqrt{Var(\hat{\beta_1})}\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Under the null hypothesis, and assuming that the error term is normally distributed, we can show that the above t-ratio follows a t-distribution with <span class="math inline">\(N-2\)</span> degrees of freedom. The decision rule is given by:</li>
</ol>
<p><span class="math display">\[|t|&gt;t_c, \  reject \ H_0\]</span></p>
<p>where <span class="math inline">\(t_c\)</span> is the critical value that can be obtained from the t-distribution table for a given level of significance.</p>

<div class="rmdNote">
<p><strong>Test of Statistical Significance</strong>: If we use <span class="math inline">\(R=0\)</span>, then the test becomes:</p>
<p><span class="math display">\[H_0: \beta_1= 0\]</span>
<span class="math display">\[H_0: \beta_1\neq 0\]</span></p>
<p>If the null hypothesis is true, then there is no statistically significant effect of X on Y. Most statistical softwares provide the t-ratio and the associated p-values for this test by default. Note that the results of this test cannot be used for inferring the economic significane of the effect of X on Y. For that the magnitude of <span class="math inline">\(\hat{\beta_1}\)</span> is more informative.</p>
</div>
<div class="example">
<p><span id="exm:unnamed-chunk-13" class="example"><strong>Example 1.2  (Test of Statistical Significance) </strong></span>Let us go back to our example of returns to education and the estimation results provided in Table <a href="simple-regression-model.html#tab:ch3table6">1.6</a>. The hypotheses for statistical significance of the effect of education on wages is given by:</p>
<p><span class="math display">\[H_0: \beta_1= 0\]</span>
<span class="math display">\[H_0: \beta_1\neq 0\]</span></p>
<p>From the estimation results, we notice that the t-ratio for the coefficient of years of schooling is 11.785. Note that the p-value is less than 0.05. Hence, using the p-value rule we will reject the null hypothesis and conclude that the effect of education on wages is <strong>statistically significant</strong>. You can also look at the critical value from the t-distribution at 5% level of significance, two-sided, and with <span class="math inline">\(N-2=998\)</span> degrees of freedom. The corresponding value is 1.96 which is less than the absolute value of the t-ratio, leading us to reject the null hypothesis as well.</p>
</div>
<div id="confidence-interval-for-regression-coefficients" class="section level3 hasAnchor" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> Confidence interval for regression coefficients<a href="simple-regression-model.html#confidence-interval-for-regression-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note that for every regression coefficient in our model, we can compute the 95% confidence interval. For example, for the slope coefficient estimate, the 95% confidence interval is given by:</p>
<p><span class="math display">\[C.I.(\hat{\beta_1}) =   \left[\hat{\beta_1}-t_{c,2-sided} \times s.e.(\hat{\beta_1}),\hat{\beta_1}+t_{c,2-sided} \times s.e.(\hat{\beta_1}) \right]\]</span></p>
<p>where where <span class="math inline">\(t_{c,2-sided}\)</span> is the critical value that can be obtained from the t-distribution table for a given level of significance (usually 5%) and degrees of freedom.</p>
<div class="example">
<p><span id="exm:unnamed-chunk-14" class="example"><strong>Example 1.3  (Confidence interval) </strong></span>Again, using the returns to education example and results from Table <a href="simple-regression-model.html#tab:ch3table6">1.6</a>, the 95% confidence interval is given by:</p>
<p><span class="math display">\[[0.106-1.96\times 0.009, 0.106+1.96\times 0.009]=[0.088,0.124]\]</span></p>
<p>Notice that becuase, <span class="math inline">\(0\)</span> is not part of the confidence interval, we can infer that the effect of education on wage is statistically significant.</p>
</div>
</div>
</div>
<div id="problems" class="section level2 unnumbered hasAnchor">
<h2>Problems<a href="simple-regression-model.html#problems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="exercise">
<p><span id="exr:unnamed-chunk-15" class="exercise"><strong>Exercise 1.1  </strong></span>Consider the following regression model:<span class="math display">\[Y_i= \beta_0 + \beta_1 X_i^3 + \epsilon_i\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Is this model considered a <strong>linear regression</strong> in parameters?</p></li>
<li><p>Write down the equation for the population regression function.</p></li>
<li><p>Suppose after using OLS you obtain: <span class="math inline">\(\hat{\beta_0}=12\)</span> and <span class="math inline">\(\hat{\beta_1}=-0.14\)</span>. Write down the equation for the predicted value.</p></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unnamed-chunk-16" class="exercise"><strong>Exercise 1.2  </strong></span>Suppose you have the following data on two variables:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(ID\)</span></th>
<th align="center"><span class="math inline">\(Y_i\)</span></th>
<th align="center"><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">7</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">9</td>
<td align="center">8</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">11</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">13</td>
<td align="center">7</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>Using OLS, compute the estimator for the slope coefficient (<span class="math inline">\(\beta_1\)</span>) and the intercept ($_0) for the following model:</li>
</ol>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Compute residuals (<span class="math inline">\(e_i\)</span>) for each observation.</p></li>
<li><p>Show that <span class="math inline">\(\sum_{i=1}^4 e_i=0\)</span> and <span class="math inline">\(\sum_{i=1}^4 e_iX_i=0\)</span>.</p></li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unnamed-chunk-17" class="exercise"><strong>Exercise 1.3  </strong></span>Suppose you have the following data on sales (Y) and advertising expenditure (X):</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(ID\)</span></th>
<th align="center"><span class="math inline">\(Y_i\)</span></th>
<th align="center"><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">8</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">10</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">12</td>
<td align="center">7</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">14</td>
<td align="center">9</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>Using OLS, compute the estimator for the slope coefficient (<span class="math inline">\(\beta_1\)</span>) and the intercept ($_0) for the following model:</li>
</ol>
<p><span class="math display">\[ln(Y_i) = \beta_0 + \beta_1 ln(X_i) + \epsilon_i\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Interpret the estimated slope coefficient.</li>
</ol>
</div>
<div class="exercise">
<p><span id="exr:unnamed-chunk-18" class="exercise"><strong>Exercise 1.4  </strong></span>Suppose you have the following regression model:</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1 ln(X_i) +\epsilon_i\]</span></p>
<p>After estimating this model in R, you obtain the following output:</p>
<table>
<thead>
<tr class="header">
<th align="center">Variable</th>
<th align="right">b</th>
<th align="right">s.e.(b)</th>
<th align="right">t-stat</th>
<th align="right">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Intercept</td>
<td align="right">-0.004</td>
<td align="right">0.003</td>
<td align="right">1.333</td>
<td align="right">0.242</td>
</tr>
<tr class="even">
<td align="center">Ln(X)</td>
<td align="right">0.887</td>
<td align="right">0.277</td>
<td align="right">3.202</td>
<td align="right">0.003</td>
</tr>
</tbody>
</table>
<p>Suppose sample size is 36 and the R-squared is 0.48.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Interpret the slope coefficients.</p></li>
<li><p>Test whether the slope coefficient is statistically significant.</p></li>
<li><p>Compute the 95% confidence interval for the slope coefficients.</p></li>
<li><p>Interpret <span class="math inline">\(R^2\)</span> of this model.</p></li>
</ol>
</div>
</div>
<div id="solutions" class="section level2 unnumbered hasAnchor">
<h2>Solutions<a href="simple-regression-model.html#solutions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Exercise 3.1:</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>Yes, because both <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> enter linearly in the model.</p></li>
<li><p>The population regression function is given by:</p></li>
</ol>
<p><span class="math display">\[E(Y_i|X_i)=\beta_0 + \beta_1 X_i^3\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>The predicted value is given by:</li>
</ol>
<p><span class="math display">\[\widehat{Y_i}=12 - 0.14 X_i^3\]</span></p>
<p><strong>Exercise 3.2:</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>Using OLS we get <span class="math inline">\(\widehat{\beta_0}=6.4\)</span> and <span class="math inline">\(\widehat{\beta_1}=0.6\)</span>.</p></li>
<li><p><span class="math inline">\(e_i=Y_i-\widehat{Y_i}=Y_i - 6.4-0.6X_i\)</span>. Hence,</p></li>
</ol>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(ID\)</span></th>
<th align="center"><span class="math inline">\(Y_i\)</span></th>
<th align="center"><span class="math inline">\(X_i\)</span></th>
<th align="center"><span class="math inline">\(e_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">7</td>
<td align="center">4</td>
<td align="center">-1.8</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">9</td>
<td align="center">8</td>
<td align="center">-2.2</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">11</td>
<td align="center">5</td>
<td align="center">1.6</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">13</td>
<td align="center">7</td>
<td align="center">2.4</td>
</tr>
</tbody>
</table>
<ol start="3" style="list-style-type: lower-alpha">
<li>It is easy to show that sum of residuals from above table is 0. Similarly you can show the sum of residuals multiplied by X is 0.</li>
</ol>
<p><strong>Exercise 3.3:</strong></p>
<ol style="list-style-type: lower-alpha">
<li>You need to first compute natural logs of both Y and X. You will get:</li>
</ol>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(ID\)</span></th>
<th align="center"><span class="math inline">\(Y_i\)</span></th>
<th align="center"><span class="math inline">\(X_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">2.08</td>
<td align="center">1.61</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">2.30</td>
<td align="center">2.20</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">2.48</td>
<td align="center">1.95</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">2.64</td>
<td align="center">2.20</td>
</tr>
</tbody>
</table>
<p>Now apply OLS to get <span class="math inline">\(\widehat{\beta_0}=1.1191\)</span> and <span class="math inline">\(\widehat{\beta_1}=0.6311\)</span>.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Because both variables are in logs, the interpretation of the slope is in terms of elasticity. Hence, if X increases by 1%, then Y increases by 0.6311%.</li>
</ol>
<p><strong>Exercise 3.4:</strong></p>
<ol style="list-style-type: lower-alpha">
<li><p>Because Y is in levels but X is in logs, the interpretation is in terms of semi-logs. So if X increases by 1 %, Y increases by 0.00887 units.</p></li>
<li><p>Using the p-value rule, the R output suggests that the coefficient of X is statisticall significant. This is because p-value is 0.003 which is less than the level of significance of 5%. Hence, we will reject the null hypothesis that the slope coefficient is equal to 0.</p></li>
<li><p>Using the confidence interval formula and critical value from t-distribution of 2.042 we get:</p></li>
</ol>
<p><span class="math display">\[CI(\beta_1): 0.887 \pm 0.277 \times 2.042=(0.3214,1.4526)\]</span></p>
<ol start="4" style="list-style-type: lower-alpha">
<li>An R-squared of 0.48 indicates the 48% of the total variation in the dependent variable is explained by our model.</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-regression-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
