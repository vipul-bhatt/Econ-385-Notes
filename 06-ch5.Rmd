# Functional form and dummy variables

Suppose we are interested in estimating a relationship between $Y$ and $X$. In general the population regression function is an unknown function of $X$, that is:

\[E(Y_i|X_i) = f(X_i)\]

Our discussion on regression model thus far has mostly focused on the linear relationship between $Y$ and $X$ variable, with the exception of using natural log transformations. In this chapter we will look at two alternative ways of capturing non-linearity in economic relationships. Our objective is to find specifications for $f(X_i)$ that allow for non-linear terms in $X$. 

## Polynomials in the regression model

One way to capture non-linear relationship between the dependent variable, $Y$, and the independent variable, $X$ is to use polynomials in $X$. For example, consider the following polyonomial of order 2 (quadratic):

\[Y_i=\beta_0 +\beta_1 X_i + \beta_2 X_i^2+\epsilon_i\]

In the above model, the first and second derivatives of $Y$ with respect to $X$ allow us to capture the non-linear relationship between these two variables. The first derivative of $Y$ with respect to $X$ gives us:

\[\frac{dY_i}{dX_i}=\beta_1 + 2 \beta_2 X_i\]

Note that:

1. The effect of $X$ on $Y$ now depends on the level of $X$.

2. We cannot interepret $\beta_1$ as the effect of $X_i$ on $Y$, holding $X_i^2$ constant. This is because when $X_i$ changes, $X_i^2$ changes as well. Hence, best way is to simply plot the non-linear relationship and/or compute the effect on predicted value of $Y$ due to change in $X$.

3. Further, taking the second derivative we get:

\[\frac{d^2Y_i}{dX_i^2}=2 \beta_2\]

Hence, the sign of $\beta_2$ tells us whether the relationship is *concave* ($\beta_2<0$) or *convex* ($\beta_2>0$). 

4. Finally, we can set the first derivative equal to $0$ and solve for $X^*$ as follows:

\[\frac{dY_i}{dX_i}=\beta_1 + 2 \beta_2 X_i= 0 \Rightarrow X^*_i=-\frac{\beta_1}{2\beta_2}\]

$X^*_i$  will either maximize or minimize $Y$ depending on the sign of $\beta_2$.

### Obtaining optimal polynomial order using goodness of fit

In general we do not know the order of polynomial and in theory can express the relationship between $Y$ and $X$ as a polynomial of order $q$ given by:

\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \beta_3 X_i^3+...+ \beta_q X_i^q + \epsilon_i\]

In order to determine what value of $q$ best explain our data we need to compare models with different number of explanatory variables. For example, to compare a quadratic model with a cubic model, we are comparing a model with two explanatory variables ($X_i$ and $X_i^2$) with a model with three independent variables ($X_i$, $X_i^2$, and $X_i^3$). Such comparisons require us to incorporate the tradeoff between improved fit and loss of degrees of freedom. In practice, do determine a value for $q$ we can follow the following procedure:

Step 1: Pick a maximum value for $q$. This part is arbitrary and usually will affect your final answer. Let us denote this value as $q_{max}$.

Step 2: Estimate the polynomial of order $q_{max}$:

\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \beta_3 X_i^3+...+ \beta_q X_i^{q_{max}} + \epsilon_i\]

This is our unrestricted model and we denote R-squared from this model as $R^2_U$.

Step 3:  Estimate the polynomial of order $q_{max}-1$

\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \beta_3 X_i^3+...+ \beta_q X_i^{q_{max}-1} + \epsilon_i\]

This is our restricted model and we denote R-squared from this model as $R^2_R$.

Step 4: Test whether there is a loss in fit between step 2 and step 3 using the F-test:

\[H_0: \text{True model is polynomial of order} \ q_{max}-1 \]
\[H_A: \text{True model is polynomial of order} \ q_{max}\]

The test statistic is given by:

\[F=\frac{(R^2_U-R^2_R)/1}{(1-R^2_U)/(N-q_{max}-1)}\]

Under the null-hypothesis, the test statistic follows F distribution with $v_1=1$ and $v_2=N-q_{max}-1$ degrees of freedom. If you reject the null hypothesis, stop and conclude that $q=q_{max}$ is the final model.

Step 5: If you do not reject the null hypothesis in Step 4, continue the process until you find the highest order polynomial where you reject the null hypothesis.

```{example}
Suppose we are interested in estimating the relationship between wages ($Y$) and labor market characteristics of workers. Specifically, we collect data on years of education ($X_1$) and years of experience ($X_2$). Consider the following specification:
  
\[Y_i= \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{2i}^2 + \epsilon_i\]

In the above model we assume that the relationship between wages  and years of experience is quadratic. We can test this formally by estimating the following restricted version of the model with only linear term in experience.

\[Y_i= \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \epsilon_i\]

If the true relationship between experience and wages is indeed quadratic, then estimating the restricted model will result in substantial loss of fit and hence will be rejected by our F-test.  Table \@ref(tab:ch5table1) below provides the estimation results of these two models. Notice that there is an drop in $R^2$ from 19.8\%, to 16\% indicating that removal of squared experience does lead to loss of fit. Next, we test whether this loss in fit is statistically significant using the F-test:
  
\[H_0: \text{True model is linear in experience} \]
\[H_A: \text{True model is quadratic in experience}\]

  \[F=\frac{(R^2_U-R^2_R)/1}{(1-R^2_U)/(N-3-1)}=\frac{0.198-0.16}{(1-0.198)/(100-3-1)}=47.19202\]

The critical value from F-table with $v_1=1$ and $v_2=1000-3-1=996$ is 3.84. Because the F statistic is greater than this critical value, we reject the null hypothesis and conclude that the relationship between wages and experience is quadratic. Also note that the coefficient of the squared term in  experience is negative implying a concave relationship between experience and wages. Finally, we can also compute the level of experience that maximizes wages by taking the first derivative with respect to experience and setttin it 0:
  
  \[\frac{dY_i}{d X_{2i}}= \beta_2 + 2\beta_3 X_{2i}=0 \Rightarrow X^*_{2i}=-\frac{\beta_1}{2\beta_2}\]

Given our estimates, this indicates that for $X^*_{2i}=-\displaystyle \frac{0.058}{2\times (-0.001)}=29$ years, we expect wages to reach a maximum with respect to experience and then level off or decline after that.
```

```{block2, type="rmdCaution"}
The use of the squared experience term is motivated by the empirical literature on determinants of earnings. As discussed earlier, the orginal Mincerian equation included a squared experience to capture the non-linear relationship between wages and experience.
```

```{r , eval=knitr::is_html_output(), message=FALSE, warning=FALSE, echo=FALSE}

library(ipumsr)

ddi = read_ipums_ddi("C:/Users/bhattvx/Dropbox/Ebooks/Econ-385-Notes/datafiles/cps_00010.xml")
data =read_ipums_micro(ddi, verbose=F)

##### drop missing  work hours

data=subset(data, UHRSWORK1!=999)
data=subset(data, UHRSWORK1!=997)
data=subset(data, UHRSWORK1!=0)

### drop missing wages

data=subset(data,INCWAGE!=9999999)
data=subset(data,INCWAGE!=9999998)
data=subset(data,INCWAGE!=0)
## generate schooling data
data$schooling=NA
data$schooling[data$EDUC==011]=1
data$schooling[data$EDUC==012]=2
data$schooling[data$EDUC==013]=3
data$schooling[data$EDUC==014]=4
data$schooling[data$EDUC==021]=5
data$schooling[data$EDUC==022]=6
data$schooling[data$EDUC==031]=7
data$schooling[data$EDUC==032]=8
data$schooling[data$EDUC==040]=9
data$schooling[data$EDUC==050]=10
data$schooling[data$EDUC==060]=11
data$schooling[data$EDUC==073]=12
data$schooling[data$EDUC==080]=13
data$schooling[data$EDUC==090]=14
data$schooling[data$EDUC==100]=15
data$schooling[data$EDUC==111]=16
data$schooling[data$EDUC==123]=18
data$schooling[data$EDUC==125]=21

### drop NA from schooling
data=na.omit(data)

### keep full time workers only

data=subset(data,UHRSWORK1>=40)

## only look at first 1000 observations
data=data[1:1000,]


y=log(data$INCWAGE)
x1=data$schooling
x2=data$AGE-data$schooling-6
x3=x2^2
fit2=lm(y~x1+x2)
fit1=lm(y~x1+x2+x3)

### regression table using sjplot package

suppressMessages(library(sjPlot))
suppressMessages(library(sjmisc))
suppressMessages(library(sjlabelled))

tab_model(fit1,fit2, digits=3, CSS = list(css.table='border: solid 2px;', css.td='border: solid 1px;',css.thead='font-weight: bold;', css.col2='text-align: center;', css.col3='text-align:center;', css.col4='text-align: center;', css.col5='text-align: center;', css.depvarhead = 'font-weight:bold;',css.firsttablerow='border-top: solid 2px;',
    css.firstsumrow= 'border-top:2px solid;'), show.ci=F, show.loglik=T,p.style="numeric", show.obs=T, show.se=T, show.stat=T,dv.labels=c("Model 1","Model 2") , pred.labels = c("Intercept","Education", "Experience", "Experience-squared"), string.p="p-value", string.se="s.e.(b)", string.est="b", string.stat="t-ratio", string.pred="Explanatory Variables", col.order = c("est", "se", "stat", "p"))
```
<caption> (\#tab:ch5table1) OLS Estimation of Earnings Equation </caption>


```{r , eval=knitr::is_latex_output(), message=FALSE, warning=FALSE, echo=FALSE, results='asis'}

library(ipumsr)

ddi = read_ipums_ddi("C:/Users/bhattvx//Dropbox/Ebooks/Econ-385-Notes/datafiles/cps_00010.xml")
data =read_ipums_micro(ddi, verbose=F)

##### drop missing  work hours

data=subset(data, UHRSWORK1!=999)
data=subset(data, UHRSWORK1!=997)
data=subset(data, UHRSWORK1!=0)

### drop missing wages

data=subset(data,INCWAGE!=9999999)
data=subset(data,INCWAGE!=9999998)
data=subset(data,INCWAGE!=0)
## generate schooling data
data$schooling=NA
data$schooling[data$EDUC==011]=1
data$schooling[data$EDUC==012]=2
data$schooling[data$EDUC==013]=3
data$schooling[data$EDUC==014]=4
data$schooling[data$EDUC==021]=5
data$schooling[data$EDUC==022]=6
data$schooling[data$EDUC==031]=7
data$schooling[data$EDUC==032]=8
data$schooling[data$EDUC==040]=9
data$schooling[data$EDUC==050]=10
data$schooling[data$EDUC==060]=11
data$schooling[data$EDUC==073]=12
data$schooling[data$EDUC==080]=13
data$schooling[data$EDUC==090]=14
data$schooling[data$EDUC==100]=15
data$schooling[data$EDUC==111]=16
data$schooling[data$EDUC==123]=18
data$schooling[data$EDUC==125]=21

### drop NA from schooling
data=na.omit(data)

### keep full time workers only

data=subset(data,UHRSWORK1>=40)

## only look at first 1000 observations
data=data[1:1000,]


y=log(data$INCWAGE)
x1=data$schooling
x2=data$AGE-data$schooling-6
x3=x2^2
fit2=lm(y~x1+x2)
fit1=lm(y~x1+x2+x3)

### regression table using sjplot package

suppressMessages(library(stargazer))

stargazer(fit1,fit2, type="latex", header=FALSE, report="vc*s", covariate.labels =c("Intercept","Education", "Experience", "Experience-squared"), dep.var.labels="Natural Log of Wages", intercept.top = T, intercept.bottom = F,title="OLS Estimation of Earnings Function" )

```



## Step functions: Dummy variables in the regression model

In many practical applications we may wish to estimate the effect of qualitative variables on an outcome of interest. For example,

a. what is the effect of a worker's occupation or race on wages? 

b. do we have an increase in consumer spending during Christmas season?

c. how does college major affects starting salary of graduates? Or


In each of the above examples, we want to estimate the effect of **group membership** on the outcome of interest. For example, do observations that belong to Christmas season have higher consumer spending than observations in non-Christmas season. One important thing to note here is that incorporating such variables in our regression mean that we need a **step-function** because we beleive there is a discrete jump in the outcome variable as we compare groups. 

By definition qualitative variables are non-numeric in nature. One way to incorporte such variables is to use **dummy variables**. A dummy variable is an indicator variable that takes value of $1$ 
if an observation belongs to a particular category and $0$ otherwise. For example, suppose we are interested in finding the effect of race on wages. Then, we can define a dummy variable, $D_i$ as follows:
\[D_i= \begin{cases}
1 & \text{if observation} \ i \ \text{is white} \\
0 & \text{otherwise}
\end{cases}\]

Once we have defined the dummy variable, we can add it to the regression model as any other independent variable. Note that:

1. A qualitative variable with $J$ categories can be incorporated by adding $J-1$ dummy variables in the regression model. The excluded category is called the **base group** and serves as the benchmark category relative to which the effect on the dependent variable is measured. For example, in the dummy variable we defined above, we have two categories in our data: white and non-white. The way we have defined the dummy variable implies that our excluded group is non-white workers. This is to avoid the *dummy variable trap*, where adding $J$ dummy variables for a qualitative variable with $J$ categories leads to perfectly linear relationship between included independent variables in our model. 

2. The effect on the regression model depends on how we add the dummy variable to our regression model. There are two options:

    2.1. Intercept dummy: here we are only interested in the categorical effect alone. This can be done by adding a dummy variable to the regression model and has the effect of shifting the intercept of the regression model.
    
    2.2. Slope dummy: here we are interested in capturing any interaction  a categorical variable may have with other indepedent variables in the regression model. This can be done by adding a product of the dummy variable with the independent variable in question and has the effect of shifting the slope of the regression model.
    
```{example, name="Intercept Dummy in Earnings Function"}
Let us continue our example of the earnings function. Now suppose we want to find out whether being male has an effect on wages,i.e,  if we compare two workers with same years of education ($X_{1}$) and experience ($X_2$), but one is male and other is female, do we see any difference in their wages. This can be done by estimating the following regression model:
  
  
\[Y_i= \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{2i}^2 + \beta_4 Male_i+ \epsilon_i\]

where, \[Male_i= \begin{cases}
1 & \text{if observation} \ i \ \text{is male} \\
0 & \text{otherwise}
\end{cases}\]

In this model, $\beta_4$  measures the effect of being male on wages for individuals with same years of education and experience. To see this, lets write down the predicted wage from our model for male and female workers separately:\[\widehat{Y^m_i}=\widehat{\beta_0} + \widehat{\beta_1} X^m_{1i} + \widehat{\beta_2} X^m_{2i} + \widehat{\beta_3} (X^m_{2i})^2 + \widehat{\beta_4}\]

\[\widehat{Y^f_i}= \widehat{\beta_0} + \widehat{\beta_1} X^f_{1i} + \widehat{\beta_2} X^f_{2i} + \widehat{\beta_3} (X^f_{2i})^2\]

Here the superscripts $m$ denotes male and $f$ denotes female. Note that if we equalize education and experience between men and women, then:
  
  \[\widehat{Y^m_i}-\widehat{Y^f_i}=\widehat{\beta_4}\]
  

Consquently, adding an intercept dummy simply changes the intercept of the regression line by the parameter of the intecept dummy. Table \@ref(tab:ch5table2) provides the estimation results for this model. Note that the estimated coefficient of Male dummy variable is 0.306. This means that a male worker's wages is 30.6\% higher than a female worker with same years of education and experience.


```{r , eval=knitr::is_html_output(), message=FALSE, warning=FALSE, echo=FALSE}

library(ipumsr)

ddi = read_ipums_ddi("C:/Users/bhattvx/Dropbox/Ebooks/Econ-385-Notes/datafiles/cps_00010.xml")
data =read_ipums_micro(ddi, verbose=F)

##### drop missing  work hours

data=subset(data, UHRSWORK1!=999)
data=subset(data, UHRSWORK1!=997)
data=subset(data, UHRSWORK1!=0)

### drop missing wages

data=subset(data,INCWAGE!=9999999)
data=subset(data,INCWAGE!=9999998)
data=subset(data,INCWAGE!=0)
## generate schooling data
data$schooling=NA
data$schooling[data$EDUC==011]=1
data$schooling[data$EDUC==012]=2
data$schooling[data$EDUC==013]=3
data$schooling[data$EDUC==014]=4
data$schooling[data$EDUC==021]=5
data$schooling[data$EDUC==022]=6
data$schooling[data$EDUC==031]=7
data$schooling[data$EDUC==032]=8
data$schooling[data$EDUC==040]=9
data$schooling[data$EDUC==050]=10
data$schooling[data$EDUC==060]=11
data$schooling[data$EDUC==073]=12
data$schooling[data$EDUC==080]=13
data$schooling[data$EDUC==090]=14
data$schooling[data$EDUC==100]=15
data$schooling[data$EDUC==111]=16
data$schooling[data$EDUC==123]=18
data$schooling[data$EDUC==125]=21

### drop NA from schooling
data=na.omit(data)

### keep full time workers only

data=subset(data,UHRSWORK1>=40)

## only look at first 1000 observations
data=data[1:1000,]


y=log(data$INCWAGE)
x1=data$schooling
x2=data$AGE-data$schooling-6
x3=x2^2
x4=as.numeric(data$SEX==1)

fit=lm(y~x1+x2+x3+x4)

### regression table using sjplot package

suppressMessages(library(sjPlot))
suppressMessages(library(sjmisc))
suppressMessages(library(sjlabelled))

tab_model(fit, digits=3, CSS = list(css.table='border: solid 2px;', css.td='border: solid 1px;',css.thead='font-weight: bold;', css.col2='text-align: center;', css.col3='text-align:center;', css.col4='text-align: center;', css.col5='text-align: center;', css.depvarhead = 'font-weight:bold;',css.firsttablerow='border-top: solid 2px;',
    css.firstsumrow= 'border-top:2px solid;'), show.ci=F, show.loglik=T,p.style="numeric", show.obs=T, show.se=T, show.stat=T,dv.labels=c("Dependent variable: Natural logs of wages") , pred.labels = c("Intercept","Education", "Experience", "Experience-squared", "Male"), string.p="p-value", string.se="s.e.(b)", string.est="b", string.stat="t-ratio", string.pred="Explanatory Variables", col.order = c("est", "se", "stat", "p"))
```
<caption> (\#tab:ch5table2) Earnings Equation  with intercept dummy variable</caption>

```{r , eval=knitr::is_latex_output(), message=FALSE, warning=FALSE, echo=FALSE, results='asis'}

library(ipumsr)

ddi = read_ipums_ddi("C:/Users/bhattvx/Dropbox/Ebooks/Econ-385-Notes/datafiles/cps_00010.xml")
data =read_ipums_micro(ddi, verbose=F)

##### drop missing  work hours

data=subset(data, UHRSWORK1!=999)
data=subset(data, UHRSWORK1!=997)
data=subset(data, UHRSWORK1!=0)

### drop missing wages

data=subset(data,INCWAGE!=9999999)
data=subset(data,INCWAGE!=9999998)
data=subset(data,INCWAGE!=0)
## generate schooling data
data$schooling=NA
data$schooling[data$EDUC==011]=1
data$schooling[data$EDUC==012]=2
data$schooling[data$EDUC==013]=3
data$schooling[data$EDUC==014]=4
data$schooling[data$EDUC==021]=5
data$schooling[data$EDUC==022]=6
data$schooling[data$EDUC==031]=7
data$schooling[data$EDUC==032]=8
data$schooling[data$EDUC==040]=9
data$schooling[data$EDUC==050]=10
data$schooling[data$EDUC==060]=11
data$schooling[data$EDUC==073]=12
data$schooling[data$EDUC==080]=13
data$schooling[data$EDUC==090]=14
data$schooling[data$EDUC==100]=15
data$schooling[data$EDUC==111]=16
data$schooling[data$EDUC==123]=18
data$schooling[data$EDUC==125]=21

### drop NA from schooling
data=na.omit(data)

### keep full time workers only

data=subset(data,UHRSWORK1>=40)

## only look at first 1000 observations
data=data[1:1000,]


y=log(data$INCWAGE)
x1=data$schooling
x2=data$AGE-data$schooling-6
x3=x2^2
x4=as.numeric(data$SEX==1)

fit=lm(y~x1+x2+x3+x4)

### regression table using sjplot package


suppressMessages(library(stargazer))

stargazer(fit1,fit2, type="latex", header=FALSE, report="vc*s", covariate.labels =c("Intercept","Education", "Experience", "Experience-squared","Male"), dep.var.labels="Natural Log of Wages", intercept.top = T, intercept.bottom = F,title="Earnings Equation  with intercept dummy variable" )

```


```{example, name="Slope Dummy in Earnings Function"}

In the previous example, we investigated whether being Male affects wages once we have controlled for education and experience. But in that analysis we assumed that there is not interaction between whether a worker is make with his years of education or experience. But is quite easy to think of interactions between the sex of a worker and their education or experience. For example, on average women tend to accumulate less years of experience than men of same age due to women taking more breaks from work due to child birth. Similarly, in many economies, men tend to have greater level of education, especially in emerging economies where parents allocate resources differentially between male and female child.  To capture such interactions, we can use slope dummy variables. Consider the following regression model:

  
  
\[Y_i= \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{2i}^2 + \beta_4 Male_i+ \beta_5 (Male_i \times X_{1i}) +\epsilon_i\]

In this model, $\beta_5$ measures the differential effect education may have on wages depending on the sex of the worker. To see this note that: 
  
\[\frac{\partial Y_i}{\partial X_{1i}}= \beta_1 + \beta_5 Male_i\]
  
As a result, for male workers, the effect of education on wages is $\beta_1 + \beta_5$ whereas for female workers this effect is $\beta_1$. Table \@ref(tab:ch5table3) provides the estimation results for this model. Note that the estimated coefficient of the interaction between Male and education is positive but statistically insignificant. Hence, in our sample, there is no evidence that sex of workers interacts with their education.


```{r , eval=knitr::is_html_output(), message=FALSE, warning=FALSE, echo=FALSE}

library(ipumsr)

ddi = read_ipums_ddi("C:/Users/bhattvx/Dropbox/Ebooks/Econ-385-Notes/datafiles/cps_00010.xml")
data =read_ipums_micro(ddi, verbose=F)

##### drop missing  work hours

data=subset(data, UHRSWORK1!=999)
data=subset(data, UHRSWORK1!=997)
data=subset(data, UHRSWORK1!=0)

### drop missing wages

data=subset(data,INCWAGE!=9999999)
data=subset(data,INCWAGE!=9999998)
data=subset(data,INCWAGE!=0)
## generate schooling data
data$schooling=NA
data$schooling[data$EDUC==011]=1
data$schooling[data$EDUC==012]=2
data$schooling[data$EDUC==013]=3
data$schooling[data$EDUC==014]=4
data$schooling[data$EDUC==021]=5
data$schooling[data$EDUC==022]=6
data$schooling[data$EDUC==031]=7
data$schooling[data$EDUC==032]=8
data$schooling[data$EDUC==040]=9
data$schooling[data$EDUC==050]=10
data$schooling[data$EDUC==060]=11
data$schooling[data$EDUC==073]=12
data$schooling[data$EDUC==080]=13
data$schooling[data$EDUC==090]=14
data$schooling[data$EDUC==100]=15
data$schooling[data$EDUC==111]=16
data$schooling[data$EDUC==123]=18
data$schooling[data$EDUC==125]=21

### drop NA from schooling
data=na.omit(data)

### keep full time workers only

data=subset(data,UHRSWORK1>=40)

## only look at first 1000 observations
data=data[1:1000,]


y=log(data$INCWAGE)
x1=data$schooling
x2=data$AGE-data$schooling-6
x3=x2^2
x4=as.numeric(data$SEX==1)
x5=x1*x4

fit=lm(y~x1+x2+x3+x4+x5)

### regression table using sjplot package

suppressMessages(library(sjPlot))
suppressMessages(library(sjmisc))
suppressMessages(library(sjlabelled))

tab_model(fit, digits=3, CSS = list(css.table='border: solid 2px;', css.td='border: solid 1px;',css.thead='font-weight: bold;', css.col2='text-align: center;', css.col3='text-align:center;', css.col4='text-align: center;', css.col5='text-align: center;', css.depvarhead = 'font-weight:bold;',css.firsttablerow='border-top: solid 2px;',
    css.firstsumrow= 'border-top:2px solid;'), show.ci=F, show.loglik=T,p.style="numeric", show.obs=T, show.se=T, show.stat=T,dv.labels=c("Dependent variable: Natural logs of wages") , pred.labels = c("Intercept","Education", "Experience", "Experience-squared", "Male", "Male*Education"), string.p="p-value", string.se="s.e.(b)", string.est="b", string.stat="t-ratio", string.pred="Explanatory Variables", col.order = c("est", "se", "stat", "p"))
```
<caption> (\#tab:ch5table3) Earnings Equation  with intercept and slope dummy variables</caption>



```{r , eval=knitr::is_latex_output(), message=FALSE, warning=FALSE, echo=FALSE, results='asis'}

library(ipumsr)

ddi = read_ipums_ddi("C:/Users/bhattvx/Dropbox/Ebooks/Econ-385-Notes/datafiles/cps_00010.xml")
data =read_ipums_micro(ddi, verbose=F)

##### drop missing  work hours

data=subset(data, UHRSWORK1!=999)
data=subset(data, UHRSWORK1!=997)
data=subset(data, UHRSWORK1!=0)

### drop missing wages

data=subset(data,INCWAGE!=9999999)
data=subset(data,INCWAGE!=9999998)
data=subset(data,INCWAGE!=0)
## generate schooling data
data$schooling=NA
data$schooling[data$EDUC==011]=1
data$schooling[data$EDUC==012]=2
data$schooling[data$EDUC==013]=3
data$schooling[data$EDUC==014]=4
data$schooling[data$EDUC==021]=5
data$schooling[data$EDUC==022]=6
data$schooling[data$EDUC==031]=7
data$schooling[data$EDUC==032]=8
data$schooling[data$EDUC==040]=9
data$schooling[data$EDUC==050]=10
data$schooling[data$EDUC==060]=11
data$schooling[data$EDUC==073]=12
data$schooling[data$EDUC==080]=13
data$schooling[data$EDUC==090]=14
data$schooling[data$EDUC==100]=15
data$schooling[data$EDUC==111]=16
data$schooling[data$EDUC==123]=18
data$schooling[data$EDUC==125]=21

### drop NA from schooling
data=na.omit(data)

### keep full time workers only

data=subset(data,UHRSWORK1>=40)

## only look at first 1000 observations
data=data[1:1000,]


y=log(data$INCWAGE)
x1=data$schooling
x2=data$AGE-data$schooling-6
x3=x2^2
x4=as.numeric(data$SEX==1)
x5=x1*x4

fit=lm(y~x1+x2+x3+x4+x5)

### regression table using sjplot package

suppressMessages(library(stargazer))

stargazer(fit1,fit2, type="latex", header=FALSE, report="vc*s", covariate.labels = c("Intercept","Education", "Experience", "Experience-squared", "Male", "Male*Education"), dep.var.labels="Natural Log of Wages", intercept.top = T, intercept.bottom = F,title="Earnings Equation  with intercept and slope dummy variables" )

```


## Problems{-}

## Solutions{-}

